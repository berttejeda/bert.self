{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Me \u00b6 Howdy. My name is Bert. I am a dedicated father and husband, and I'm sure I'm a pretty regular guy (for now). My trade is tech, with a specialty in Cloud Architecture, Systems Administration/Orchestration, and everything that ships with DevOps methodoligies, which includes Desired State and Configuration Management, among other buzzwordy terms. My tech stack \u00b6 I absolutely love DevOps and am forever a student of programming languages such as Python , PowerShell , Go , and Ruby . For shell scripting on POSIX-compliant systems, I mostly use Bash and zsh , but I can get pretty crafty on Windows with PowerShell and even BATCH . My Web skills includes: Frontend Development React , CSS Good ol' javascript Backend/API Development Flask nodejs My goto for infrastructure orchestration and platform tooling is Ansible . For Infrastructure as Code (IaC), I use Terraform . Microservices and Virtualization? Oh yeah!: Kubernetes Rancher Docker Ovirt / RHEV ProxMox VMware But wait, there's more: I'm all about that Jenkins and Argo CD for Continuous Integration/Continuous Delivery (CI/CD), and I love, love, love these Atlassian Products: JIRA Confluence Bitbucket My Tech Notes \u00b6 All of my notes are organized by main topic. You can use the search bar up top for real-time search results or click the hamburger menu at the top left to navigate through a given topic. Enjoy, and happy learning!","title":"Home"},{"location":"#about-me","text":"Howdy. My name is Bert. I am a dedicated father and husband, and I'm sure I'm a pretty regular guy (for now). My trade is tech, with a specialty in Cloud Architecture, Systems Administration/Orchestration, and everything that ships with DevOps methodoligies, which includes Desired State and Configuration Management, among other buzzwordy terms.","title":"About Me"},{"location":"#my-tech-stack","text":"I absolutely love DevOps and am forever a student of programming languages such as Python , PowerShell , Go , and Ruby . For shell scripting on POSIX-compliant systems, I mostly use Bash and zsh , but I can get pretty crafty on Windows with PowerShell and even BATCH . My Web skills includes: Frontend Development React , CSS Good ol' javascript Backend/API Development Flask nodejs My goto for infrastructure orchestration and platform tooling is Ansible . For Infrastructure as Code (IaC), I use Terraform . Microservices and Virtualization? Oh yeah!: Kubernetes Rancher Docker Ovirt / RHEV ProxMox VMware But wait, there's more: I'm all about that Jenkins and Argo CD for Continuous Integration/Continuous Delivery (CI/CD), and I love, love, love these Atlassian Products: JIRA Confluence Bitbucket","title":"My tech stack"},{"location":"#my-tech-notes","text":"All of my notes are organized by main topic. You can use the search bar up top for real-time search results or click the hamburger menu at the top left to navigate through a given topic. Enjoy, and happy learning!","title":"My Tech Notes"},{"location":"tags/","text":"Tags \u00b6 [TAGS]","title":"Tags"},{"location":"tags/#tags","text":"[TAGS]","title":"Tags"},{"location":"docs/","text":"","title":"Index"},{"location":"docs/about-mkdocs/","text":"Overview \u00b6 What is MkDocs? \u00b6 The description on the MkDocs site is: Project documentation with Markdown. MkDocs is a Python tool that generates a static site based on content written in Markdown. If you are new to markdown, see the Getting Started page on the Markdown Guide website. Resources \u00b6 mkdocs.org homepage mkdocs/mkdocs MkDocs Wiki - covering themes, plugins, recipes and more. Release notes for MkDocs. Reasons to use MkDocs \u00b6 Create an elegant, modern docs site for your project. Create a static site and serve from GitHub Pages easily. Low-code solution No need to write HTML or learn templating syntax needed Use your existing markdown files as content. Configure with a simple YAML file. Customizable. Add custom HTML if you want. Plugins available. Flexible theme choices. Includes search by default. Broken links to files (including from your navbar) will be detected at build time and shown as warnings. Do I need to know Python? \u00b6 MkDocs is built in Python (like Sphinx), but you don't have to write Python code. If you set up a Deploy flow right, you don't even have to set it up locally, though then you can't preview.","title":"Overview"},{"location":"docs/about-mkdocs/#overview","text":"","title":"Overview"},{"location":"docs/about-mkdocs/#what-is-mkdocs","text":"The description on the MkDocs site is: Project documentation with Markdown. MkDocs is a Python tool that generates a static site based on content written in Markdown. If you are new to markdown, see the Getting Started page on the Markdown Guide website.","title":"What is MkDocs?"},{"location":"docs/about-mkdocs/#resources","text":"mkdocs.org homepage mkdocs/mkdocs MkDocs Wiki - covering themes, plugins, recipes and more. Release notes for MkDocs.","title":"Resources"},{"location":"docs/about-mkdocs/#reasons-to-use-mkdocs","text":"Create an elegant, modern docs site for your project. Create a static site and serve from GitHub Pages easily. Low-code solution No need to write HTML or learn templating syntax needed Use your existing markdown files as content. Configure with a simple YAML file. Customizable. Add custom HTML if you want. Plugins available. Flexible theme choices. Includes search by default. Broken links to files (including from your navbar) will be detected at build time and shown as warnings.","title":"Reasons to use MkDocs"},{"location":"docs/about-mkdocs/#do-i-need-to-know-python","text":"MkDocs is built in Python (like Sphinx), but you don't have to write Python code. If you set up a Deploy flow right, you don't even have to set it up locally, though then you can't preview.","title":"Do I need to know Python?"},{"location":"docs/topics/ansible/lesson-01/lesson/","text":"Forward \u00b6 Thank you for taking the time to read through this interactive document; this being the first of 3 parts. I hope these hands-on, interactive lessons can reduce the startup cost of learning and eventually mastering Ansible . The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Lesson 01 \u00b6 Here's what I will cover for Lab 1 of this Ansible tutorial series: What is ansible? How to install Ansible on Windows/Linux/MacOS How to prepare a test environment for Ansible using Docker Let's begin. What is Ansible? \u00b6 Ansible is a simple yet powerful tool for configuration management and orchestration of your infrastructure. It speeds up installing software, configuring servers, and most importantly reduces manual, error-prone methods for managing modern infrastructure components. It is also a great alternative to Puppet and Chef . Both are similar tools to Ansible, but in my opinion Ansible is much easier to learn and master. In a nutshell, Ansible : Is like a higher-level, idempotent version of traditional shell scripts Is much, much easier to rapidly develop and manage, since its automation is (mostly) defined using yaml Lends itself to self-documenting development Terminology \u00b6 Throughout the sections ahead, I'll make reference to the following ansible terms: inventory - This is how ansible is made aware of the machines it is to manage playbooks - This is the ansible version of a bash script tasks - Think of this as you would a neatly commented step in a bash script, only far more superior in structure and clarity variables - Just like bash variables, ansible stores repeatable information in these named constructs facts - Very similar to variables, but think more of these as values that are automatically derived from a machine or from a little bit of ansible magic For more information on ansible terms, consult the Ansible Glossary Let's go over installing Ansible Installing Ansible \u00b6 Debian Systems \u00b6 Installation on Ubuntu 20.04 LTS : sudo apt-get update sudo apt-get install -y curl software-properties-common || sudo apt-get install -y python-software-properties sudo apt-get -y autoremove sudo apt-get install -y --allow-unauthenticated python-setuptools python-dev libffi-dev libssl-dev git sshpass tree sudo apt-get -y install python-pip sudo pip install ansible cryptography RHEL Systems \u00b6 Installation on CentOS 8.x , Oracle Enterprise Linux 8.x rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm yum install -y ansible In the next sections, we'll cover writing and launching ansible playbooks. My editor of choice is Sublime 3 Text Editor , but we'll be using vi throughout this lab. Writing ansible playbooks \u00b6 The fun starts when you learn your way around ansible playbooks ! Let's create some of these, as Follows: Create a folder for your playbook: mkdir -p sandbox Create a playbook under this path named hello.yaml touch sandbox/hello.yaml Add the playbook definition with a single debug task: echo -e \"\"\" - hosts: localhost connection: local tasks: - name: debug | Say Hello! debug: msg: | Hello from the imported playbook! \"\"\" >> sandbox/hello.yaml Run the playbook, specifying your inventory as 'localhost': ansible-playbook -i localhost, sandbox/hello.yaml This concludes lab1. In lesson-02, we'll be converting a complicated bash shell script into an Ansible Playbook. Lesson-02 will cover: Playbooks and Tasks Templates and Handlers Variables & Facts Task Blocks","title":"lesson-01"},{"location":"docs/topics/ansible/lesson-01/lesson/#forward","text":"Thank you for taking the time to read through this interactive document; this being the first of 3 parts. I hope these hands-on, interactive lessons can reduce the startup cost of learning and eventually mastering Ansible .","title":"Forward"},{"location":"docs/topics/ansible/lesson-01/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/ansible/lesson-01/lesson/#lesson-01","text":"Here's what I will cover for Lab 1 of this Ansible tutorial series: What is ansible? How to install Ansible on Windows/Linux/MacOS How to prepare a test environment for Ansible using Docker Let's begin.","title":"Lesson 01"},{"location":"docs/topics/ansible/lesson-01/lesson/#what-is-ansible","text":"Ansible is a simple yet powerful tool for configuration management and orchestration of your infrastructure. It speeds up installing software, configuring servers, and most importantly reduces manual, error-prone methods for managing modern infrastructure components. It is also a great alternative to Puppet and Chef . Both are similar tools to Ansible, but in my opinion Ansible is much easier to learn and master. In a nutshell, Ansible : Is like a higher-level, idempotent version of traditional shell scripts Is much, much easier to rapidly develop and manage, since its automation is (mostly) defined using yaml Lends itself to self-documenting development","title":"What is Ansible?"},{"location":"docs/topics/ansible/lesson-01/lesson/#terminology","text":"Throughout the sections ahead, I'll make reference to the following ansible terms: inventory - This is how ansible is made aware of the machines it is to manage playbooks - This is the ansible version of a bash script tasks - Think of this as you would a neatly commented step in a bash script, only far more superior in structure and clarity variables - Just like bash variables, ansible stores repeatable information in these named constructs facts - Very similar to variables, but think more of these as values that are automatically derived from a machine or from a little bit of ansible magic For more information on ansible terms, consult the Ansible Glossary Let's go over installing Ansible","title":"Terminology"},{"location":"docs/topics/ansible/lesson-01/lesson/#installing-ansible","text":"","title":"Installing Ansible"},{"location":"docs/topics/ansible/lesson-01/lesson/#debian-systems","text":"Installation on Ubuntu 20.04 LTS : sudo apt-get update sudo apt-get install -y curl software-properties-common || sudo apt-get install -y python-software-properties sudo apt-get -y autoremove sudo apt-get install -y --allow-unauthenticated python-setuptools python-dev libffi-dev libssl-dev git sshpass tree sudo apt-get -y install python-pip sudo pip install ansible cryptography","title":"Debian Systems"},{"location":"docs/topics/ansible/lesson-01/lesson/#rhel-systems","text":"Installation on CentOS 8.x , Oracle Enterprise Linux 8.x rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm yum install -y ansible In the next sections, we'll cover writing and launching ansible playbooks. My editor of choice is Sublime 3 Text Editor , but we'll be using vi throughout this lab.","title":"RHEL Systems"},{"location":"docs/topics/ansible/lesson-01/lesson/#writing-ansible-playbooks","text":"The fun starts when you learn your way around ansible playbooks ! Let's create some of these, as Follows: Create a folder for your playbook: mkdir -p sandbox Create a playbook under this path named hello.yaml touch sandbox/hello.yaml Add the playbook definition with a single debug task: echo -e \"\"\" - hosts: localhost connection: local tasks: - name: debug | Say Hello! debug: msg: | Hello from the imported playbook! \"\"\" >> sandbox/hello.yaml Run the playbook, specifying your inventory as 'localhost': ansible-playbook -i localhost, sandbox/hello.yaml This concludes lab1. In lesson-02, we'll be converting a complicated bash shell script into an Ansible Playbook. Lesson-02 will cover: Playbooks and Tasks Templates and Handlers Variables & Facts Task Blocks","title":"Writing ansible playbooks"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/","tags":["ansible","windows","cygwin"],"text":"Overview \u00b6 You want to install ansible on Windows 7 x64 Environment Information \u00b6 Windows OS: powershell -NoProfile [System.Environment]::OSVersion.Version OS: Windows 7 x64 Major Minor Build Revision ----- ----- ----- -------- 6 1 7601 65536 Cygwin Version (post-installation): 3.0.7(0.338/5/3) Ansible Environment (post-installation): ansible --version ansible 2.9.0.dev0 config file = None configured module search path = [u'/home/${USERNAME}/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible-2.9.0.dev0-py2.7.egg/ansible executable location = /opt/ansible/bin/ansible python version = 2.7.16 (default, Mar 20 2019, 12:15:19) [GCC 7.4.0] Instructions \u00b6 Install cygwin \u00b6 Download cygwin64 from https://www.cygwin.com/setup-x86_64.exe Create the installation folder (I'm using C:\\tools\\cygwin ) Move setup-x86_64.exe to C:\\tools\\cygwin Click setup-x86_64.exe Make sure to set the install folder and package folder to C:\\tools\\cygwin Go through the setup instructions, and when at the package selection screen, Only install wget The cygwin64 core package installations will take time, so busy yourself with something else in the meantime :) Once the installation is complete, close it out Install python and its dependencies \u00b6 From the installation folder ( C:\\tools\\cygwin ), click Cygwin.bat This will initialize your cygwin environment and start the bash interactive terminal Adjust shell environment: export PATH=\"/usr/bin:$PATH\" PATH+=:~+/bin Install the apt-cyg package manager wget raw.github.com/transcode-open/apt-cyg/master/apt-cyg chmod +x apt-cyg mv apt-cyg /usr/local/bin which -a apt-cyg >/dev/null 2>&1 && echo ok install git, python-devel, gcc-g++, curl, dos2unix, zip, unzip apt-cyg install git python-devel curl dos2unix zip unzip install pip wget https://bootstrap.pypa.io/get-pip.py python get-pip.py Install python and its dependencies \u00b6 Your cygwin environment should be good for installing ansible, so let's get to it. Install dependence apt-cyg install openssl openssl-devel libffi-devel vim apt-cyg install python-{jinja2,six,yaml,crypto,cryptography} Install ansible from github mkdir /opt cd /opt git clone --depth 1 git://github.com/ansible/ansible cd ansible python setup.py install Update your PATH variable echo export PATH=\"/opt/ansible/bin:\\$PATH\" >> ~/.bash_profile Verify installation which -a ansible >/dev/null 2>&1 && echo We found the anible binary! || echo We could not find the ansible binary! Please troubleshoot! Troubleshooting If problems, use a search engine to look up any errors, start over, rinse/repeat Test Run \u00b6 The below worked! ansible --connection=local localhost, -m setup -a 'filter=ansible_host*' Appendix \u00b6 Setup cmder \u00b6 I ran the above tests using Cmder . Instructions: - Start cmder - Click the green + sign at the bottom right - Click \"Setup Tasks\" - Click the + sign at the bottom of 'Predefined tasks (command groups)' - For 'Task Parameters', enter: /icon \"**C:\\tools\\cygwin**\\Cygwin.ico\" - Paste this into the empty text area at the bottom right: -cur_console:t:ansible \"**C:\\tools\\cygwin**\\bin\\bash.exe\" --login -i Notes \u00b6 Final folder size for C:\\tools\\cygwin : 1.24 GB That's a big-ass folder for lil' ol' ansible ... So yeah ... Learning Points \u00b6 Ansible can (at least in theory) run on Windows 7 x64, but it's not without pain AND it's not without problems. I tested the uri module and was greeted with an error: The following modules failed to execute: setup setup: MODULE FAILURE See stdout/stderr for the exact error More digging I must do. References \u00b6 Google Search > ansible on windows 7 - https://lmgtfy.com/?q=ansible+on+windows+7 Google Search > \"apt-cyg\" \"ansible\" \"bat\" - https://lmgtfy.com/?q=%22apt-cyg%22+%22ansible%22+%22bat%22 pip - Installing Ansible Python package on Windows - Stack Overflow - https://stackoverflow.com/questions/51167099/installing-ansible-python-package-on-windows Running Ansible within Windows (Jeff Geerling) - https://www.jeffgeerling.com/blog/running-ansible-within-windows How to install Ansible to Windows - https://gist.github.com/eyasuyuki/d9c1dc96a9b8356164e5","title":"Portable Ansible Installation on Windows 7 using Cygwin"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#overview","text":"You want to install ansible on Windows 7 x64","title":"Overview"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#environment-information","text":"Windows OS: powershell -NoProfile [System.Environment]::OSVersion.Version OS: Windows 7 x64 Major Minor Build Revision ----- ----- ----- -------- 6 1 7601 65536 Cygwin Version (post-installation): 3.0.7(0.338/5/3) Ansible Environment (post-installation): ansible --version ansible 2.9.0.dev0 config file = None configured module search path = [u'/home/${USERNAME}/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible-2.9.0.dev0-py2.7.egg/ansible executable location = /opt/ansible/bin/ansible python version = 2.7.16 (default, Mar 20 2019, 12:15:19) [GCC 7.4.0]","title":"Environment Information"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#instructions","text":"","title":"Instructions"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#install-cygwin","text":"Download cygwin64 from https://www.cygwin.com/setup-x86_64.exe Create the installation folder (I'm using C:\\tools\\cygwin ) Move setup-x86_64.exe to C:\\tools\\cygwin Click setup-x86_64.exe Make sure to set the install folder and package folder to C:\\tools\\cygwin Go through the setup instructions, and when at the package selection screen, Only install wget The cygwin64 core package installations will take time, so busy yourself with something else in the meantime :) Once the installation is complete, close it out","title":"Install cygwin"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#install-python-and-its-dependencies","text":"From the installation folder ( C:\\tools\\cygwin ), click Cygwin.bat This will initialize your cygwin environment and start the bash interactive terminal Adjust shell environment: export PATH=\"/usr/bin:$PATH\" PATH+=:~+/bin Install the apt-cyg package manager wget raw.github.com/transcode-open/apt-cyg/master/apt-cyg chmod +x apt-cyg mv apt-cyg /usr/local/bin which -a apt-cyg >/dev/null 2>&1 && echo ok install git, python-devel, gcc-g++, curl, dos2unix, zip, unzip apt-cyg install git python-devel curl dos2unix zip unzip install pip wget https://bootstrap.pypa.io/get-pip.py python get-pip.py","title":"Install python and its dependencies"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#install-python-and-its-dependencies_1","text":"Your cygwin environment should be good for installing ansible, so let's get to it. Install dependence apt-cyg install openssl openssl-devel libffi-devel vim apt-cyg install python-{jinja2,six,yaml,crypto,cryptography} Install ansible from github mkdir /opt cd /opt git clone --depth 1 git://github.com/ansible/ansible cd ansible python setup.py install Update your PATH variable echo export PATH=\"/opt/ansible/bin:\\$PATH\" >> ~/.bash_profile Verify installation which -a ansible >/dev/null 2>&1 && echo We found the anible binary! || echo We could not find the ansible binary! Please troubleshoot! Troubleshooting If problems, use a search engine to look up any errors, start over, rinse/repeat","title":"Install python and its dependencies"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#test-run","text":"The below worked! ansible --connection=local localhost, -m setup -a 'filter=ansible_host*'","title":"Test Run"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#appendix","text":"","title":"Appendix"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#setup-cmder","text":"I ran the above tests using Cmder . Instructions: - Start cmder - Click the green + sign at the bottom right - Click \"Setup Tasks\" - Click the + sign at the bottom of 'Predefined tasks (command groups)' - For 'Task Parameters', enter: /icon \"**C:\\tools\\cygwin**\\Cygwin.ico\" - Paste this into the empty text area at the bottom right: -cur_console:t:ansible \"**C:\\tools\\cygwin**\\bin\\bash.exe\" --login -i","title":"Setup cmder"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#notes","text":"Final folder size for C:\\tools\\cygwin : 1.24 GB That's a big-ass folder for lil' ol' ansible ... So yeah ...","title":"Notes"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#learning-points","text":"Ansible can (at least in theory) run on Windows 7 x64, but it's not without pain AND it's not without problems. I tested the uri module and was greeted with an error: The following modules failed to execute: setup setup: MODULE FAILURE See stdout/stderr for the exact error More digging I must do.","title":"Learning Points"},{"location":"docs/topics/ansible/windows/ansible-on-windows-7-with-cygwin/#references","text":"Google Search > ansible on windows 7 - https://lmgtfy.com/?q=ansible+on+windows+7 Google Search > \"apt-cyg\" \"ansible\" \"bat\" - https://lmgtfy.com/?q=%22apt-cyg%22+%22ansible%22+%22bat%22 pip - Installing Ansible Python package on Windows - Stack Overflow - https://stackoverflow.com/questions/51167099/installing-ansible-python-package-on-windows Running Ansible within Windows (Jeff Geerling) - https://www.jeffgeerling.com/blog/running-ansible-within-windows How to install Ansible to Windows - https://gist.github.com/eyasuyuki/d9c1dc96a9b8356164e5","title":"References"},{"location":"docs/topics/crossplane/lesson-01/lesson/","text":"Forward \u00b6 Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning CrossPlane . The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Lesson 01 \u00b6 This lesson will cover the following exercises: Initialize a local Kubernetes cluster using kind Install CrossPlane on a local cluster using helm Connect the installation to an AWS account What is Kubernetes? \u00b6 Kubernetes is a microservices architecture (MSA) platform for automating deployment, operations and scaling of containerized applications. It can run anywhere where Linux runs and supports on-premise, hybrid and public cloud deployments. What is CrossPlane? \u00b6 CrossPlane is a free, open-source Kubernetes add-on that transforms your cluster into a universal control plane Was created by Upbound, and was first released in December of 2018 It was accepted as an incubating project by the CNCF (Cloud Native Computing Foundation) in 2020 CrossPlane Features \u00b6 From the website : CrossPlane enables platform teams to assemble infrastructure from multiple vendors, and expose higher level self-service APIs for application teams to consume, without having to write any code. CrossPlane extends your Kubernetes cluster to support orchestrating any infrastructure or managed services Can be installed into any Kubernetes cluster to get started Supports most major cloud providers and covers typical service deployments As such, it offers an alternative to Terraform, CDK, and Pulumi Why is this approach to infrastructure management significant? \u00b6 I mentioned self-service in the previous slide. That's the key here. From a deployment standpoint, Kubernetes already breaks down barriers to entry for Developers To get an app deployed, developers need only describe their workloads using Kubernetes API Documents, which are expressed in yaml syntax CrossPlane extends this self-service approach a layer above that -- to the infrastructure itself It helps Developers claim cloud resources through expression of Kubernetes API Documents in the same way they do for their workloads The infrastructure is defined declaratively without writing any code and without revealing the underlying infrastructure of the specific vendor Because CrossPlane exposes administration of higher-level infrastructure in a Kubernetes-native manner, it allows us to easily design, manage, distribute, and consume these infrastructure abstractions within the existing ecosystem of Kubernetes add-ons, plugins, and integrations Let's put what I just said into practice Exercise 1 - Create a local Kubernetes cluster \u00b6 Since CrossPlane is a Kubernets add-on, we'll need a Kubernetes cluster on which to install it. For the purposes of this introduction, we'll utilize kind to initialize our demo cluster. For those of you not familiar with the tool, it's used for running local Kubernetes clusters using Docker containers as worker nodes . I've already installed version v0.14.0 of the tool: kind version The syntax for creating a Kubernetes cluster using the kind command is quite simple. We can run kind create cluster --help to get more info, but you can create a cluster by just running kind create cluster . For our demonstration, we'll create a Kubernetes cluster named crossplane-demo with some customizations echo -e \"\"\" kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - role: worker \"\"\" | kind create cluster --name crossplane-demo --config=- Next, we'll initialize our Kubernetes config and context. You can retrieve your cluster's kubeconfig via the command kind --name crossplane-demo get kubeconfig We'll pipe that to our kubeconfig file using the tee command: kind --name crossplane-demo get kubeconfig | tee ~/.kube/crossplane.yaml We'll then set our KUBECONFIG environmental variable: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') and finally set our active Kubernetes context: kubectl config use-context kind-crossplane-demo Next, we install CrossPlane. Exercise 2 - Install CrossPlane \u00b6 Create the crossplane namespace: kubectl create namespace crossplane-system We'll install its components using helm : First, add the crossplane helm repo: helm repo add crossplane-stable https://charts.crossplane.io/stable Sync our local helm repos with their upstream helm repo update We install crossplane: helm install crossplane --namespace crossplane-system crossplane-stable/crossplane Lastly, we check the components are up and healthy: kubectl get all -n crossplane-system Now that we have CrossPlane installed, let's connect it to our AWS infrastructure. Exercise 3 - Install the AWS Provider \u00b6 For CrossPlane to manage our AWS Infrastructure, we must first install the AWS Provider . A CrossPlane provider ships with CRDs ( Custom Resources Definitions ) required to create resources on the AWS cloud. Create the provider Kubernetes API Document: echo -e \"\"\"apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: aws-provider spec: package: crossplane/provider-aws:alpha\"\"\" > provider.yaml Apply the document: kubectl apply -f provider.yaml Wait for the Provider to become healthy: kubectl get provider.pkg --watch Now that the Provider is in a healthy state, we're ready to connect CrossPlane to our AWS infrastructure. Exercise 4 - Connect CrossPlane to your AWS Infrastructure \u00b6 To connect CrossPlane to your AWS Infrastructure, you must create a ProviderConfig definition. I've already configured my AWS credentials using aws configure beforehand, so I'll now generate the Provider configuration file with my AWS Credentials, as follows: echo -e \"\"\" [default] aws_access_key_id = $(aws configure get aws_access_key_id --profile default) aws_secret_access_key = $(aws configure get aws_secret_access_key --profile default) \"\"\" > creds.conf Next, we create a Kubernetes secret with the configuration file we just generated: kubectl create secret generic aws-secret-creds -n crossplane-system --from-file=creds=./creds.conf We then create the Provider config for our AWS account: echo -e \"\"\"apiVersion: aws.crossplane.io/v1beta1 kind: ProviderConfig metadata: name: awsconfig spec: credentials: source: Secret secretRef: namespace: crossplane-system name: aws-secret-creds key: creds\"\"\" > provider-config.yaml Now apply our provider config: kubectl apply -f provider-config.yaml Because we're ultimately working with Kubernetes-native resources, we can verify the provider's pod instance: kubectl -n crossplane-system get pods -l pkg.crossplane.io/provider=provider-aws Everything looks good so far With CrossPlane installed and connected to our AWS infrastructure, we're ready to launch some nukes. Let's move onto lesson 02.","title":"lesson-01"},{"location":"docs/topics/crossplane/lesson-01/lesson/#forward","text":"Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning CrossPlane .","title":"Forward"},{"location":"docs/topics/crossplane/lesson-01/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/crossplane/lesson-01/lesson/#lesson-01","text":"This lesson will cover the following exercises: Initialize a local Kubernetes cluster using kind Install CrossPlane on a local cluster using helm Connect the installation to an AWS account","title":"Lesson 01"},{"location":"docs/topics/crossplane/lesson-01/lesson/#what-is-kubernetes","text":"Kubernetes is a microservices architecture (MSA) platform for automating deployment, operations and scaling of containerized applications. It can run anywhere where Linux runs and supports on-premise, hybrid and public cloud deployments.","title":"What is Kubernetes?"},{"location":"docs/topics/crossplane/lesson-01/lesson/#what-is-crossplane","text":"CrossPlane is a free, open-source Kubernetes add-on that transforms your cluster into a universal control plane Was created by Upbound, and was first released in December of 2018 It was accepted as an incubating project by the CNCF (Cloud Native Computing Foundation) in 2020","title":"What is CrossPlane?"},{"location":"docs/topics/crossplane/lesson-01/lesson/#crossplane-features","text":"From the website : CrossPlane enables platform teams to assemble infrastructure from multiple vendors, and expose higher level self-service APIs for application teams to consume, without having to write any code. CrossPlane extends your Kubernetes cluster to support orchestrating any infrastructure or managed services Can be installed into any Kubernetes cluster to get started Supports most major cloud providers and covers typical service deployments As such, it offers an alternative to Terraform, CDK, and Pulumi","title":"CrossPlane Features"},{"location":"docs/topics/crossplane/lesson-01/lesson/#why-is-this-approach-to-infrastructure-management-significant","text":"I mentioned self-service in the previous slide. That's the key here. From a deployment standpoint, Kubernetes already breaks down barriers to entry for Developers To get an app deployed, developers need only describe their workloads using Kubernetes API Documents, which are expressed in yaml syntax CrossPlane extends this self-service approach a layer above that -- to the infrastructure itself It helps Developers claim cloud resources through expression of Kubernetes API Documents in the same way they do for their workloads The infrastructure is defined declaratively without writing any code and without revealing the underlying infrastructure of the specific vendor Because CrossPlane exposes administration of higher-level infrastructure in a Kubernetes-native manner, it allows us to easily design, manage, distribute, and consume these infrastructure abstractions within the existing ecosystem of Kubernetes add-ons, plugins, and integrations Let's put what I just said into practice","title":"Why is this approach to infrastructure management significant?"},{"location":"docs/topics/crossplane/lesson-01/lesson/#exercise-1-create-a-local-kubernetes-cluster","text":"Since CrossPlane is a Kubernets add-on, we'll need a Kubernetes cluster on which to install it. For the purposes of this introduction, we'll utilize kind to initialize our demo cluster. For those of you not familiar with the tool, it's used for running local Kubernetes clusters using Docker containers as worker nodes . I've already installed version v0.14.0 of the tool: kind version The syntax for creating a Kubernetes cluster using the kind command is quite simple. We can run kind create cluster --help to get more info, but you can create a cluster by just running kind create cluster . For our demonstration, we'll create a Kubernetes cluster named crossplane-demo with some customizations echo -e \"\"\" kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - role: worker \"\"\" | kind create cluster --name crossplane-demo --config=- Next, we'll initialize our Kubernetes config and context. You can retrieve your cluster's kubeconfig via the command kind --name crossplane-demo get kubeconfig We'll pipe that to our kubeconfig file using the tee command: kind --name crossplane-demo get kubeconfig | tee ~/.kube/crossplane.yaml We'll then set our KUBECONFIG environmental variable: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') and finally set our active Kubernetes context: kubectl config use-context kind-crossplane-demo Next, we install CrossPlane.","title":"Exercise 1 - Create a local Kubernetes cluster"},{"location":"docs/topics/crossplane/lesson-01/lesson/#exercise-2-install-crossplane","text":"Create the crossplane namespace: kubectl create namespace crossplane-system We'll install its components using helm : First, add the crossplane helm repo: helm repo add crossplane-stable https://charts.crossplane.io/stable Sync our local helm repos with their upstream helm repo update We install crossplane: helm install crossplane --namespace crossplane-system crossplane-stable/crossplane Lastly, we check the components are up and healthy: kubectl get all -n crossplane-system Now that we have CrossPlane installed, let's connect it to our AWS infrastructure.","title":"Exercise 2 - Install CrossPlane"},{"location":"docs/topics/crossplane/lesson-01/lesson/#exercise-3-install-the-aws-provider","text":"For CrossPlane to manage our AWS Infrastructure, we must first install the AWS Provider . A CrossPlane provider ships with CRDs ( Custom Resources Definitions ) required to create resources on the AWS cloud. Create the provider Kubernetes API Document: echo -e \"\"\"apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: aws-provider spec: package: crossplane/provider-aws:alpha\"\"\" > provider.yaml Apply the document: kubectl apply -f provider.yaml Wait for the Provider to become healthy: kubectl get provider.pkg --watch Now that the Provider is in a healthy state, we're ready to connect CrossPlane to our AWS infrastructure.","title":"Exercise 3 - Install the AWS Provider"},{"location":"docs/topics/crossplane/lesson-01/lesson/#exercise-4-connect-crossplane-to-your-aws-infrastructure","text":"To connect CrossPlane to your AWS Infrastructure, you must create a ProviderConfig definition. I've already configured my AWS credentials using aws configure beforehand, so I'll now generate the Provider configuration file with my AWS Credentials, as follows: echo -e \"\"\" [default] aws_access_key_id = $(aws configure get aws_access_key_id --profile default) aws_secret_access_key = $(aws configure get aws_secret_access_key --profile default) \"\"\" > creds.conf Next, we create a Kubernetes secret with the configuration file we just generated: kubectl create secret generic aws-secret-creds -n crossplane-system --from-file=creds=./creds.conf We then create the Provider config for our AWS account: echo -e \"\"\"apiVersion: aws.crossplane.io/v1beta1 kind: ProviderConfig metadata: name: awsconfig spec: credentials: source: Secret secretRef: namespace: crossplane-system name: aws-secret-creds key: creds\"\"\" > provider-config.yaml Now apply our provider config: kubectl apply -f provider-config.yaml Because we're ultimately working with Kubernetes-native resources, we can verify the provider's pod instance: kubectl -n crossplane-system get pods -l pkg.crossplane.io/provider=provider-aws Everything looks good so far With CrossPlane installed and connected to our AWS infrastructure, we're ready to launch some nukes. Let's move onto lesson 02.","title":"Exercise 4 - Connect CrossPlane to your AWS Infrastructure"},{"location":"docs/topics/crossplane/lesson-02/lesson/","text":"Forward \u00b6 Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning CrossPlane . The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Lesson 02 \u00b6 In the previous lesson we: Initialized a local Kubernetes cluster using kind Installed CrossPlane on the cluster using helm Connected the installation to an AWS account This lesson will cover the following exercises: Install an EKS cluster using Crossplane, which entails: Creating Kubernetes API Documents for: VPC IAM Roles Security Groups Subnet VPC Internet Gateway VPC NAT Gateway Route Table Cluster EKS Node Group Clone the lesson materials \u00b6 This lesson calls several Kubernetes API Documents to create the EKS cluster. These can be found in my lessons repo on Github: https://github.com/berttejeda/bert.lessons.git Let's perform a sparse clone to gather the lesson materials before we start: mkdir bert.lessons cd bert.lessons git init git remote add -f origin https://github.com/berttejeda/bert.lessons.git git config core.sparseCheckout true echo 'crossplane/lesson-02/eks' >> .git/info/sparse-checkout git pull origin main cd crossplane/lesson-02/eks clear Set Your Kubernetes Context \u00b6 Let's set our KUBECONFIG environmental variable as before: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') Switch to the Kubernetes context from the previous lesson: kubectl config use-context kind-crossplane-demo Exercise 1 - Create the AWS Resources for our EKS cluster \u00b6 Create the VPC and verify its state: kubectl apply -f vpc/vpc.yaml Verify the state of the VPC resource: kubectl get vpc Create the security groups for the EKS cluster and Node Groups: kubectl apply -f security/securitygroup.yaml Verify that these resources are Synced and Ready: kubectl get securitygroup Create the subnet resources: kubectl apply -f networking/subnet.yaml Verify Synced and Ready: kubectl get subnet Create the Elastic IP resources: kubectl apply -f networking/elasticip.yaml Verify Synced and Ready: kubectl get address Create the Internet and Nat gateway resources: kubectl apply -f networking/gateway.yaml Verify that these resources are Synced and Ready: kubectl get internetgateway kubectl get natgateway Create the IaM role and attach appropriate policies for our EKS cluster: kubectl apply -f security/iam.yaml Verify Synced and Ready: kubectl get role Create the eks control plane: kubectl apply -f cluster/control-plane.yaml Verify Synced and Ready: kubectl get cluster.eks.aws.crossplane.io/cp-demo-k8s-cluster Create the EKS worker node group: kubectl apply -f cluster/workers.yaml Verify Synced and Ready: kubectl get nodegroup At this point, the EKS cluster should in a Creating state. It can take up to 20 minutes or so for the cluster to be created. That we utilized a Kubernetes-native approach to provisioning the above infrastructure is immensely powerful. If you're a dev, you don't need to learn a new technology stack to gain insight and understanding on how to provision the compute resources you need to deploy your workloads. Closing Notes \u00b6 Here's a github gist for doing the above in Terraform: https://gist.github.com/zparnold/dba28e5022e8c029919e0be3c38c64cc Although the underlying actions are identical, the implementation is not as self-service oriented as with CrossPlane. Here's why in a nutshell: CrossPlane leverages Kubernetes, which ships with RBAC Kubernetes has a rich ecosystem of custom resources, integrations, tooling, etc which all play nice with CrossPlane, because it's a Kubernetes add-on after all The only downside I see so far is that it's not as widely known as tools like Terraform, and so there are not as many examples on the internet hive mind as one would like. Still, I am increasingly seeing documentation available from AWS official sources, so I believe it's only a matter of time before CrossPlane really takes off.","title":"lesson-02"},{"location":"docs/topics/crossplane/lesson-02/lesson/#forward","text":"Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning CrossPlane .","title":"Forward"},{"location":"docs/topics/crossplane/lesson-02/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/crossplane/lesson-02/lesson/#lesson-02","text":"In the previous lesson we: Initialized a local Kubernetes cluster using kind Installed CrossPlane on the cluster using helm Connected the installation to an AWS account This lesson will cover the following exercises: Install an EKS cluster using Crossplane, which entails: Creating Kubernetes API Documents for: VPC IAM Roles Security Groups Subnet VPC Internet Gateway VPC NAT Gateway Route Table Cluster EKS Node Group","title":"Lesson 02"},{"location":"docs/topics/crossplane/lesson-02/lesson/#clone-the-lesson-materials","text":"This lesson calls several Kubernetes API Documents to create the EKS cluster. These can be found in my lessons repo on Github: https://github.com/berttejeda/bert.lessons.git Let's perform a sparse clone to gather the lesson materials before we start: mkdir bert.lessons cd bert.lessons git init git remote add -f origin https://github.com/berttejeda/bert.lessons.git git config core.sparseCheckout true echo 'crossplane/lesson-02/eks' >> .git/info/sparse-checkout git pull origin main cd crossplane/lesson-02/eks clear","title":"Clone the lesson materials"},{"location":"docs/topics/crossplane/lesson-02/lesson/#set-your-kubernetes-context","text":"Let's set our KUBECONFIG environmental variable as before: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') Switch to the Kubernetes context from the previous lesson: kubectl config use-context kind-crossplane-demo","title":"Set Your Kubernetes Context"},{"location":"docs/topics/crossplane/lesson-02/lesson/#exercise-1-create-the-aws-resources-for-our-eks-cluster","text":"Create the VPC and verify its state: kubectl apply -f vpc/vpc.yaml Verify the state of the VPC resource: kubectl get vpc Create the security groups for the EKS cluster and Node Groups: kubectl apply -f security/securitygroup.yaml Verify that these resources are Synced and Ready: kubectl get securitygroup Create the subnet resources: kubectl apply -f networking/subnet.yaml Verify Synced and Ready: kubectl get subnet Create the Elastic IP resources: kubectl apply -f networking/elasticip.yaml Verify Synced and Ready: kubectl get address Create the Internet and Nat gateway resources: kubectl apply -f networking/gateway.yaml Verify that these resources are Synced and Ready: kubectl get internetgateway kubectl get natgateway Create the IaM role and attach appropriate policies for our EKS cluster: kubectl apply -f security/iam.yaml Verify Synced and Ready: kubectl get role Create the eks control plane: kubectl apply -f cluster/control-plane.yaml Verify Synced and Ready: kubectl get cluster.eks.aws.crossplane.io/cp-demo-k8s-cluster Create the EKS worker node group: kubectl apply -f cluster/workers.yaml Verify Synced and Ready: kubectl get nodegroup At this point, the EKS cluster should in a Creating state. It can take up to 20 minutes or so for the cluster to be created. That we utilized a Kubernetes-native approach to provisioning the above infrastructure is immensely powerful. If you're a dev, you don't need to learn a new technology stack to gain insight and understanding on how to provision the compute resources you need to deploy your workloads.","title":"Exercise 1 - Create the AWS Resources for our EKS cluster"},{"location":"docs/topics/crossplane/lesson-02/lesson/#closing-notes","text":"Here's a github gist for doing the above in Terraform: https://gist.github.com/zparnold/dba28e5022e8c029919e0be3c38c64cc Although the underlying actions are identical, the implementation is not as self-service oriented as with CrossPlane. Here's why in a nutshell: CrossPlane leverages Kubernetes, which ships with RBAC Kubernetes has a rich ecosystem of custom resources, integrations, tooling, etc which all play nice with CrossPlane, because it's a Kubernetes add-on after all The only downside I see so far is that it's not as widely known as tools like Terraform, and so there are not as many examples on the internet hive mind as one would like. Still, I am increasingly seeing documentation available from AWS official sources, so I believe it's only a matter of time before CrossPlane really takes off.","title":"Closing Notes"},{"location":"docs/topics/docker/lesson-01/lesson/","text":"Forward \u00b6 Thank you for taking the time to read through this interactive document; this being the first of 4 parts. I hope these hands-on, interactive lessons can reduce the startup cost of learning and eventually mastering Docker . {% if session is defined -%} Welcome {{ session['environment'].get('USERNAME') }} This lab serves as a basic introduction to use of the Docker container runtime. The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Software Requirements \u00b6 The main software required to follow this workshop is Docker. Installation \u00b6 Docker is supported on many architectures. Head over to docker.com for installation instructions. Exercise 1 - Hello, world \u00b6 Docker containers are in a sense similar to chrooted processes, but they also provide many more features that we are going to explore. Let's run a \"hello world\" example: docker run --name busybox-echo busybox echo \"hello world\" A breakdown of the above command: docker # Docker client binary used to interact with Docker run # Docker subcommand - runs a command in a container --name busybox-echo # Assign the container the name of busybox-echo busybox # container image used by the run command echo \"hello world\" # actual command to run (and arguments) Container images carry within themselves all the needed libraries, binaries and directories in order to be able to run. TIP: Container images could be abstracted as \"the blueprint for an object\", while containers themselves are the actualization of the object into a real instance/entity. Exercise 2 - Interacting with running containers \u00b6 You can list running containers with: docker ps Here's an example showing the likely output from the ps command: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES The fields shown in the output can be summarized as: Container ID - auto generated unique running id image - image name Command - Linux process running as the PID 1 in the container Names - user friendly name of the container As you'll note, upon running the \"hello world\" example in Excercise 1 , you may not see any running containers. This is expected since the entire life cycle of the command echo \"hello world\" has already finished and thus the container has stopped. Once the command running inside the container finishes its execution, the container will stop running, but will still be available, even if it's not listed in the output of ps by default. Exercise 3 - List all containers, including stopped ones \u00b6 To list both running and stopped containers, issue the -a flag when invoking docker ps , as with: docker ps -a The output from the ps command above is likely to be similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0395e6f047a8 busybox \"echo 'hello world'\" 8 seconds ago Exited (0) 7 seconds ago busybox-echo Stopped containers will remain available until cleaned, so let's clean this up in the next step. Remove stopped containers \u00b6 To remove stopper containers, issue the docker rm command, as with: docker rm my_container_name_or_id The argument used for the rm command can be the container ID or the container name, e.g. docker rm 0395e6f047a8 docker rm busybox-echo If you prefer, it's possible to add the option --rm to the run subcommand so that the container will be cleaned automatically as soon as it stops its execution, as with: docker run --name busybox-echo --rm busybox echo \"hello world\" Excercise 4 - Work with container Environment Variables \u00b6 Let's go over the container shell environment Inspect the environment variables in a conatiner \u00b6 docker run --name busybox-echo --rm busybox env You should see output similar to: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=0a0169cdec9a HOME=/root Important: The environment variables passed to the container may be different on other systems and the hostname is randomized per container, unless specified differently. Extend the container's environment by passing variable flags as docker run arguments \u00b6 You can introduce additional environment variables to a container via the -e flag, as with: docker run --name busybox-echo --rm -e HELLO=world busybox env You should see output similar to: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=0a0169cdec9a HOME=/root HELLO=world Excercise 5 - Container Processes \u00b6 Inspect a container's process tree \u00b6 docker run --name busybox-echo --rm busybox ps uax You should see output similar to: PID USER TIME COMMAND 1 root 0:00 ps uax Does this mean we are running this command as root? Technically yes, although remember, as we anticipated, this is not the actual root of your host system but a very limited one running inside the container. We will get back to the topic of users and security a bit later. In fact, as you can see, the process runs in a very limited and isolated environment where it cannot see or access all the other processes running on your machine. Exercise 6 - Adding host mounts \u00b6 Inspect a container filesystem \u00b6 The filesystem used inside running containers is also isolated and separated from that of the host, as illustrated: docker run --name busybox-echo --rm busybox ls -l /home What if we want to expose a directory on the host to a container or vice versa? To do so, the option -v/--volume must be used Expose the current directory to the container \u00b6 docker run --name busybox-echo --rm -v $PWD:/home busybox ls -l /home In this example, the current directory, specified via $PWD, was \"mounted\" from the host system in the container so that it appeared to be \"/home\" inside the container! Expose one or more directories inside a container \u00b6 To expose multiple directories the option -v/--volume must be chained mkdir -p ~/somedir touch ~/somedir/somefile{1,2,3} docker run --name busybox-echo --rm -v $PWD:/home -v ~/somedir:/somedir busybox ls -l /home /somedir Excercise 7 - Working with Container Networking \u00b6 Inspect a container's network interfaces \u00b6 Networking in Docker containers is also isolated. Let's look at the interfaces inside a running container docker run --name busybox-echo --rm busybox ifconfig Now, let's demonstrate a container running a simple HTTP echo server Exercise 8 - Run a simple http server inside a container \u00b6 docker run --name busybox-echo -i -t --rm -p 8080:8080 busybox \\ sh -c 'while true; do \\ { echo -e \"HTTP/1.1 200 OK\\r\\n\"; echo \"busybox!\"; } | \\ nc -l -p 8080; \\ done' Let's review the structure of the command we just used: docker # Docker client binary used to interact with Docker run # Docker subcommand - runs a command in a container --name busybox-echo # Assign the container the name of busybox-echo -i # Run container interactively, that is, Keep STDIN open even if not attached -t # Run container and Allocate a pseudo-TTY --rm # Automatically remove the container when it exits -p 8080:8080 # Forward port 8080 on the host to port 8080 in the container busybox # container image used by the run command sh -c ... # invoke the Bourne shell (sh) with the while loop as the command This command remains alive and attached to the current session because the busybox echo server will keep listening for requests. Try reaching it from a different terminal via the following command: curl http://127.0.0.1:8080 You should see output similar to: 'busybox!' Press Ctrl-C in the terminal running the container to stop it. Excercise 9 - Daemons, a.k.a detached containers \u00b6 Our last echo server example was inconvenient as it worked in foreground so it was bound to our shell. As you noted, if we closed our shell, the container would also die with it. Let's fix this problem by changing our docker run command Start a detached container \u00b6 docker run --name busybox-echo -d --rm -p 8080:8080 busybox \\ sh -c 'while true; do \\ { echo -e \"HTTP/1.1 200 OK\\r\\n\"; echo \"busybox!\"; } | \\ nc -l -p 8080; \\ done' The -d flag instructs Docker to start the process in the background. Let's see if our HTTP connection still works after we close our session: curl http://127.0.0.1:8080 Output should be similar to: busybox! It's still working and now we can see it running with the ps command: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTSNAMES af0cb3d329b3 busybox \"sh -c 'while true; ...\" 2 seconds ago Up 2 seconds 0.0.0.0:8080->8080/tcp busybox-echo Excercise 10 - View logs for a running container \u00b6 If we want more information about a running container we can check its logs output using the logs command: docker logs busybox-echo Docker also offers the useful command inspect which retrieves all the info related to a specific object (network, container, image, ecc): Exercise 11 - Inspect a container's metadata \u00b6 docker inspect busybox-echo Output should be similar to: [ { \"Id\": \"9899fe8be722739ea9b155cba1699d9df86af0d7384bd757b101dea6195a25d5\", \"Created\": \"2020-04-17T20:42:23.080028292Z\", \"Path\": \"sh\", \"Args\": [ \"-c\", \"while true; ... ], \"State\": { \"Status\": \"running\", ... Exercise 12 - Attach to a running container \u00b6 While a container is still running, we can enter its namespaces using the exec command docker exec -it busybox-echo sh The command above will open an sh interactive shell that we can use to peer inside the container. As with the run command from before, -t flag attaches terminal for interactive typing -i flag attaches input/output from the terminal to the process Inspect processes inside the running container \u00b6 If you followed along without problems, you should not be inside the container from the previous exercise ps uax Now that we have opened a new shell inside the container, let's find what process is running as PID 1. This workflow is similar to using SSH to access a remote computer's commandline terminal. The difference here is that when connecting to the container, there is no remote network connection involved. The sh process is a shell session that is started in the container's namespaces instead of those of the host OS. ps uax Output should be similar to: PID USER TIME COMMAND 1 root 0:00 sh -c while true; ... 10 root 0:00 nc -l -p 8080 16 root 0:00 sh 21 root 0:00 ps aux Detach from the container by pressing ctrl +d Exercise 13 - Attaching to a container's input \u00b6 To best illustrate the impact of -i, or --interactive in the expanded version, consider this example echo \"hello there\" | docker run --rm busybox grep hello The example above won't work as the container's input is not attached to the host stdout. The -i flag fixes just that: echo \"hello there\" | docker run --rm -i busybox grep hello Output should be similar to: hello there Excercise 14 - Starting and stopping containers \u00b6 It is possible to stop and start long-living containers using stop and start commands docker stop busybox-echo You've stopped the busybox container we've been playing with. To start it up again, simply run: docker start busybox-echo The container will start up with its previously defined startup arguments.","title":"lesson-01"},{"location":"docs/topics/docker/lesson-01/lesson/#forward","text":"Thank you for taking the time to read through this interactive document; this being the first of 4 parts. I hope these hands-on, interactive lessons can reduce the startup cost of learning and eventually mastering Docker . {% if session is defined -%} Welcome {{ session['environment'].get('USERNAME') }} This lab serves as a basic introduction to use of the Docker container runtime.","title":"Forward"},{"location":"docs/topics/docker/lesson-01/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/docker/lesson-01/lesson/#software-requirements","text":"The main software required to follow this workshop is Docker.","title":"Software Requirements"},{"location":"docs/topics/docker/lesson-01/lesson/#installation","text":"Docker is supported on many architectures. Head over to docker.com for installation instructions.","title":"Installation"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-1-hello-world","text":"Docker containers are in a sense similar to chrooted processes, but they also provide many more features that we are going to explore. Let's run a \"hello world\" example: docker run --name busybox-echo busybox echo \"hello world\" A breakdown of the above command: docker # Docker client binary used to interact with Docker run # Docker subcommand - runs a command in a container --name busybox-echo # Assign the container the name of busybox-echo busybox # container image used by the run command echo \"hello world\" # actual command to run (and arguments) Container images carry within themselves all the needed libraries, binaries and directories in order to be able to run. TIP: Container images could be abstracted as \"the blueprint for an object\", while containers themselves are the actualization of the object into a real instance/entity.","title":"Exercise 1 - Hello, world"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-2-interacting-with-running-containers","text":"You can list running containers with: docker ps Here's an example showing the likely output from the ps command: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES The fields shown in the output can be summarized as: Container ID - auto generated unique running id image - image name Command - Linux process running as the PID 1 in the container Names - user friendly name of the container As you'll note, upon running the \"hello world\" example in Excercise 1 , you may not see any running containers. This is expected since the entire life cycle of the command echo \"hello world\" has already finished and thus the container has stopped. Once the command running inside the container finishes its execution, the container will stop running, but will still be available, even if it's not listed in the output of ps by default.","title":"Exercise 2 - Interacting with running containers"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-3-list-all-containers-including-stopped-ones","text":"To list both running and stopped containers, issue the -a flag when invoking docker ps , as with: docker ps -a The output from the ps command above is likely to be similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0395e6f047a8 busybox \"echo 'hello world'\" 8 seconds ago Exited (0) 7 seconds ago busybox-echo Stopped containers will remain available until cleaned, so let's clean this up in the next step.","title":"Exercise 3 - List all containers, including stopped ones"},{"location":"docs/topics/docker/lesson-01/lesson/#remove-stopped-containers","text":"To remove stopper containers, issue the docker rm command, as with: docker rm my_container_name_or_id The argument used for the rm command can be the container ID or the container name, e.g. docker rm 0395e6f047a8 docker rm busybox-echo If you prefer, it's possible to add the option --rm to the run subcommand so that the container will be cleaned automatically as soon as it stops its execution, as with: docker run --name busybox-echo --rm busybox echo \"hello world\"","title":"Remove stopped containers"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-4-work-with-container-environment-variables","text":"Let's go over the container shell environment","title":"Excercise 4 - Work with container Environment Variables"},{"location":"docs/topics/docker/lesson-01/lesson/#inspect-the-environment-variables-in-a-conatiner","text":"docker run --name busybox-echo --rm busybox env You should see output similar to: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=0a0169cdec9a HOME=/root Important: The environment variables passed to the container may be different on other systems and the hostname is randomized per container, unless specified differently.","title":"Inspect the environment variables in a conatiner"},{"location":"docs/topics/docker/lesson-01/lesson/#extend-the-containers-environment-by-passing-variable-flags-as-docker-run-arguments","text":"You can introduce additional environment variables to a container via the -e flag, as with: docker run --name busybox-echo --rm -e HELLO=world busybox env You should see output similar to: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=0a0169cdec9a HOME=/root HELLO=world","title":"Extend the container's environment by passing variable flags as docker run arguments"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-5-container-processes","text":"","title":"Excercise 5 - Container Processes"},{"location":"docs/topics/docker/lesson-01/lesson/#inspect-a-containers-process-tree","text":"docker run --name busybox-echo --rm busybox ps uax You should see output similar to: PID USER TIME COMMAND 1 root 0:00 ps uax Does this mean we are running this command as root? Technically yes, although remember, as we anticipated, this is not the actual root of your host system but a very limited one running inside the container. We will get back to the topic of users and security a bit later. In fact, as you can see, the process runs in a very limited and isolated environment where it cannot see or access all the other processes running on your machine.","title":"Inspect a container's process tree"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-6-adding-host-mounts","text":"","title":"Exercise 6 - Adding host mounts"},{"location":"docs/topics/docker/lesson-01/lesson/#inspect-a-container-filesystem","text":"The filesystem used inside running containers is also isolated and separated from that of the host, as illustrated: docker run --name busybox-echo --rm busybox ls -l /home What if we want to expose a directory on the host to a container or vice versa? To do so, the option -v/--volume must be used","title":"Inspect a container filesystem"},{"location":"docs/topics/docker/lesson-01/lesson/#expose-the-current-directory-to-the-container","text":"docker run --name busybox-echo --rm -v $PWD:/home busybox ls -l /home In this example, the current directory, specified via $PWD, was \"mounted\" from the host system in the container so that it appeared to be \"/home\" inside the container!","title":"Expose the current directory to the container"},{"location":"docs/topics/docker/lesson-01/lesson/#expose-one-or-more-directories-inside-a-container","text":"To expose multiple directories the option -v/--volume must be chained mkdir -p ~/somedir touch ~/somedir/somefile{1,2,3} docker run --name busybox-echo --rm -v $PWD:/home -v ~/somedir:/somedir busybox ls -l /home /somedir","title":"Expose one or more directories inside a container"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-7-working-with-container-networking","text":"","title":"Excercise 7 - Working with Container Networking"},{"location":"docs/topics/docker/lesson-01/lesson/#inspect-a-containers-network-interfaces","text":"Networking in Docker containers is also isolated. Let's look at the interfaces inside a running container docker run --name busybox-echo --rm busybox ifconfig Now, let's demonstrate a container running a simple HTTP echo server","title":"Inspect a container's network interfaces"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-8-run-a-simple-http-server-inside-a-container","text":"docker run --name busybox-echo -i -t --rm -p 8080:8080 busybox \\ sh -c 'while true; do \\ { echo -e \"HTTP/1.1 200 OK\\r\\n\"; echo \"busybox!\"; } | \\ nc -l -p 8080; \\ done' Let's review the structure of the command we just used: docker # Docker client binary used to interact with Docker run # Docker subcommand - runs a command in a container --name busybox-echo # Assign the container the name of busybox-echo -i # Run container interactively, that is, Keep STDIN open even if not attached -t # Run container and Allocate a pseudo-TTY --rm # Automatically remove the container when it exits -p 8080:8080 # Forward port 8080 on the host to port 8080 in the container busybox # container image used by the run command sh -c ... # invoke the Bourne shell (sh) with the while loop as the command This command remains alive and attached to the current session because the busybox echo server will keep listening for requests. Try reaching it from a different terminal via the following command: curl http://127.0.0.1:8080 You should see output similar to: 'busybox!' Press Ctrl-C in the terminal running the container to stop it.","title":"Exercise 8 - Run a simple http server inside a container"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-9-daemons-aka-detached-containers","text":"Our last echo server example was inconvenient as it worked in foreground so it was bound to our shell. As you noted, if we closed our shell, the container would also die with it. Let's fix this problem by changing our docker run command","title":"Excercise 9 - Daemons, a.k.a detached containers"},{"location":"docs/topics/docker/lesson-01/lesson/#start-a-detached-container","text":"docker run --name busybox-echo -d --rm -p 8080:8080 busybox \\ sh -c 'while true; do \\ { echo -e \"HTTP/1.1 200 OK\\r\\n\"; echo \"busybox!\"; } | \\ nc -l -p 8080; \\ done' The -d flag instructs Docker to start the process in the background. Let's see if our HTTP connection still works after we close our session: curl http://127.0.0.1:8080 Output should be similar to: busybox! It's still working and now we can see it running with the ps command: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTSNAMES af0cb3d329b3 busybox \"sh -c 'while true; ...\" 2 seconds ago Up 2 seconds 0.0.0.0:8080->8080/tcp busybox-echo","title":"Start a detached container"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-10-view-logs-for-a-running-container","text":"If we want more information about a running container we can check its logs output using the logs command: docker logs busybox-echo Docker also offers the useful command inspect which retrieves all the info related to a specific object (network, container, image, ecc):","title":"Excercise 10 - View logs for a running container"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-11-inspect-a-containers-metadata","text":"docker inspect busybox-echo Output should be similar to: [ { \"Id\": \"9899fe8be722739ea9b155cba1699d9df86af0d7384bd757b101dea6195a25d5\", \"Created\": \"2020-04-17T20:42:23.080028292Z\", \"Path\": \"sh\", \"Args\": [ \"-c\", \"while true; ... ], \"State\": { \"Status\": \"running\", ...","title":"Exercise 11 - Inspect a container's metadata"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-12-attach-to-a-running-container","text":"While a container is still running, we can enter its namespaces using the exec command docker exec -it busybox-echo sh The command above will open an sh interactive shell that we can use to peer inside the container. As with the run command from before, -t flag attaches terminal for interactive typing -i flag attaches input/output from the terminal to the process","title":"Exercise 12 - Attach to a running container"},{"location":"docs/topics/docker/lesson-01/lesson/#inspect-processes-inside-the-running-container","text":"If you followed along without problems, you should not be inside the container from the previous exercise ps uax Now that we have opened a new shell inside the container, let's find what process is running as PID 1. This workflow is similar to using SSH to access a remote computer's commandline terminal. The difference here is that when connecting to the container, there is no remote network connection involved. The sh process is a shell session that is started in the container's namespaces instead of those of the host OS. ps uax Output should be similar to: PID USER TIME COMMAND 1 root 0:00 sh -c while true; ... 10 root 0:00 nc -l -p 8080 16 root 0:00 sh 21 root 0:00 ps aux Detach from the container by pressing ctrl +d","title":"Inspect processes inside the running container"},{"location":"docs/topics/docker/lesson-01/lesson/#exercise-13-attaching-to-a-containers-input","text":"To best illustrate the impact of -i, or --interactive in the expanded version, consider this example echo \"hello there\" | docker run --rm busybox grep hello The example above won't work as the container's input is not attached to the host stdout. The -i flag fixes just that: echo \"hello there\" | docker run --rm -i busybox grep hello Output should be similar to: hello there","title":"Exercise 13 - Attaching to a container's input"},{"location":"docs/topics/docker/lesson-01/lesson/#excercise-14-starting-and-stopping-containers","text":"It is possible to stop and start long-living containers using stop and start commands docker stop busybox-echo You've stopped the busybox container we've been playing with. To start it up again, simply run: docker start busybox-echo The container will start up with its previously defined startup arguments.","title":"Excercise 14 - Starting and stopping containers"},{"location":"docs/topics/docker/lesson-02/lesson/","text":"Forward \u00b6 So far we have been using container images downloaded from Docker's public registry. One of the key success factors for Docker among competitors was the possibility to easily create, customize, share and improve container images cooperatively. This lab covers this feature in detail. The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Exercise 1 - Starting from scratch \u00b6 If we want to start from scratch, let's build the ingredients for our container image. Create a hello world script \u00b6 mkdir docker-workspace cd docker-workspace echo -e '''#!/bin/sh echo \"hello, world $@\" ''' > hello.sh Create your Dockerfile \u00b6 A Dockerfile is a special file that instructs the docker build command on how to build an image echo -e '''FROM scratch ADD hello.sh /hello.sh ''' > Dockerfile You should now have in your directory two files, Dockerfile and hello.sh Build your image \u00b6 docker build -t hello . The output should be similar to: Sending build context to Docker daemon 3.072 kB Step 1/2 : FROM scratch ---> Step 2/2 : ADD hello.sh /hello.sh ---> 03899b124d13 Successfully built 03899b124d13 Successfully tagged hello:latest The Dockerfile used is very simple: FROM scratch ADD hello.sh /hello.sh FROM scratch instructs the Docker build process to use an empty image as the basis to build our custom container image ADD hello.sh /hello.sh adds the file hello.sh to the container's root path /hello.sh. Exercise 2 - Viewing images \u00b6 Let's go over viewing docker images List available docker images \u00b6 Run the command docker images to display images that we have built. Running the above command should display image information similar to: REPOSITORY TAG IMAGE ID CREATED SIZE hello latest fcb780e01f47 5 minutes ago 35B Here's a quick explanation of the columns shown in that output: Repository - a name associated to this image locally (on your computer) or on a remote repository. Our current repository is local and the image is called hello Tag - indicates the version of our image, Docker sets the latest tag automatically if none is specified Image ID - unique image ID Size - the size of our image is very small (~35 bytes) NOTE: Docker images are quite different from virtual machine image formats. Since Docker does not boot any operating system, but simply runs Linux processes in isolation, we don't need any kernel-level objects or drivers to ship with an image, so the size can potentially be as tiny as just a few bytes! Exercise 3 - Running our image \u00b6 Let's go over running our image, i.e. invoking our image as a container ... Run the newly-built image \u00b6 docker run --rm hello /hello.sh As you'll note, trying to run our newly built image will result in an error similar to one of the following (depending on your version of Docker): write pipe: bad file descriptor or standard_init_linux.go:211: exec user process caused \"no such file or directory\" This is because our container is empty. There is no shell, so the script won't be able to start! Let's fix that by changing our base image to busybox , as it contains a proper shell environment ... Exercise 4 - Rebuilding the image \u00b6 Time to rebuild our docker image. Recreate your Dockerfile using busybox as your base image \u00b6 echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ''' > Dockerfile Build your container again \u00b6 docker build -t hello . The output should be similar to: Sending build context to Docker daemon 6.144kB Step 1/3 : FROM busybox ---> be5888e67be6 Step 2/3 : ADD hello.sh /hello.sh ---> Using cache ---> a54d28b10018 Step 3/3 : RUN chmod +x /hello.sh ---> Using cache ---> befa8400f441 Successfully built befa8400f441 Successfully tagged hello:latest Again, the Dockerfile used is very simple: FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh FROM busybox instructs the Docker build process to use the busybox public docker image as the basis to build our custom container image ADD hello.sh /hello.sh adds the file hello.sh to the container's root path /hello.sh. RUN chmod +x /hello.sh sets the execution bit on the hello.sh script to make it executable. Listing the image should show that image ID and size have changed. List available docker images \u00b6 Running docker images should produce output similar to: REPOSITORY TAG IMAGE ID CREATED SIZE hello latest 2a56d704ff5b 43 seconds ago 1.22MB <none> <none> f94453b3174d 9 minutes ago 34B busybox latest be5888e67be6 2 weeks ago 1.22MB We should be able to run our container now. Run your rebuilt image \u00b6 docker run --rm hello /hello.sh <%= $env:Username %> Output should be similar to: hello, world <%= $env:Username %> Exercise 5 - Versioning your image \u00b6 Let's roll out a new version of our script Modify your hello world script \u00b6 echo -e '''#!/bin/sh echo \"hello, world $@\" echo \"I am a new image\" ''' > hello.sh We don't need to update our Dockerfile, as it references the same script. We just have to rebuild the image. Build your image again, this time with a new tag \u00b6 docker build -t hello:v2 . Run your v2 image \u00b6 docker run --rm hello:v2 /hello.sh <%= $env:Username %> Output should be similar to: hello, world <%= $env:Username %> I'm a new image Exercise 6 - Entry points \u00b6 We can improve our image by supplying an entrypoint. This will set the default command executed if none is specified when starting the container. Define an entrypoint for your image \u00b6 echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile Build a new version of your image \u00b6 docker build -t hello:v3 . We should now be able to run the new image version without supplying additional arguments Run your v3 image \u00b6 docker run --rm hello:v3 Output should be similar to: hello, world I am a new image What happens if you pass an additional argument as in previous examples? These will be passed to the ENTRYPOINT command as arguments. Run your v3 image with arguments \u00b6 docker run --rm hello:v3 testing Read more: workshop/docker.md at master \u00b7 gravitational/workshop","title":"lesson-02"},{"location":"docs/topics/docker/lesson-02/lesson/#forward","text":"So far we have been using container images downloaded from Docker's public registry. One of the key success factors for Docker among competitors was the possibility to easily create, customize, share and improve container images cooperatively. This lab covers this feature in detail.","title":"Forward"},{"location":"docs/topics/docker/lesson-02/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-1-starting-from-scratch","text":"If we want to start from scratch, let's build the ingredients for our container image.","title":"Exercise 1 - Starting from scratch"},{"location":"docs/topics/docker/lesson-02/lesson/#create-a-hello-world-script","text":"mkdir docker-workspace cd docker-workspace echo -e '''#!/bin/sh echo \"hello, world $@\" ''' > hello.sh","title":"Create a hello world script"},{"location":"docs/topics/docker/lesson-02/lesson/#create-your-dockerfile","text":"A Dockerfile is a special file that instructs the docker build command on how to build an image echo -e '''FROM scratch ADD hello.sh /hello.sh ''' > Dockerfile You should now have in your directory two files, Dockerfile and hello.sh","title":"Create your Dockerfile"},{"location":"docs/topics/docker/lesson-02/lesson/#build-your-image","text":"docker build -t hello . The output should be similar to: Sending build context to Docker daemon 3.072 kB Step 1/2 : FROM scratch ---> Step 2/2 : ADD hello.sh /hello.sh ---> 03899b124d13 Successfully built 03899b124d13 Successfully tagged hello:latest The Dockerfile used is very simple: FROM scratch ADD hello.sh /hello.sh FROM scratch instructs the Docker build process to use an empty image as the basis to build our custom container image ADD hello.sh /hello.sh adds the file hello.sh to the container's root path /hello.sh.","title":"Build your image"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-2-viewing-images","text":"Let's go over viewing docker images","title":"Exercise 2 - Viewing images"},{"location":"docs/topics/docker/lesson-02/lesson/#list-available-docker-images","text":"Run the command docker images to display images that we have built. Running the above command should display image information similar to: REPOSITORY TAG IMAGE ID CREATED SIZE hello latest fcb780e01f47 5 minutes ago 35B Here's a quick explanation of the columns shown in that output: Repository - a name associated to this image locally (on your computer) or on a remote repository. Our current repository is local and the image is called hello Tag - indicates the version of our image, Docker sets the latest tag automatically if none is specified Image ID - unique image ID Size - the size of our image is very small (~35 bytes) NOTE: Docker images are quite different from virtual machine image formats. Since Docker does not boot any operating system, but simply runs Linux processes in isolation, we don't need any kernel-level objects or drivers to ship with an image, so the size can potentially be as tiny as just a few bytes!","title":"List available docker images"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-3-running-our-image","text":"Let's go over running our image, i.e. invoking our image as a container ...","title":"Exercise 3 - Running our image"},{"location":"docs/topics/docker/lesson-02/lesson/#run-the-newly-built-image","text":"docker run --rm hello /hello.sh As you'll note, trying to run our newly built image will result in an error similar to one of the following (depending on your version of Docker): write pipe: bad file descriptor or standard_init_linux.go:211: exec user process caused \"no such file or directory\" This is because our container is empty. There is no shell, so the script won't be able to start! Let's fix that by changing our base image to busybox , as it contains a proper shell environment ...","title":"Run the newly-built image"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-4-rebuilding-the-image","text":"Time to rebuild our docker image.","title":"Exercise 4 - Rebuilding the image"},{"location":"docs/topics/docker/lesson-02/lesson/#recreate-your-dockerfile-using-busybox-as-your-base-image","text":"echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ''' > Dockerfile","title":"Recreate your Dockerfile using busybox as your base image"},{"location":"docs/topics/docker/lesson-02/lesson/#build-your-container-again","text":"docker build -t hello . The output should be similar to: Sending build context to Docker daemon 6.144kB Step 1/3 : FROM busybox ---> be5888e67be6 Step 2/3 : ADD hello.sh /hello.sh ---> Using cache ---> a54d28b10018 Step 3/3 : RUN chmod +x /hello.sh ---> Using cache ---> befa8400f441 Successfully built befa8400f441 Successfully tagged hello:latest Again, the Dockerfile used is very simple: FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh FROM busybox instructs the Docker build process to use the busybox public docker image as the basis to build our custom container image ADD hello.sh /hello.sh adds the file hello.sh to the container's root path /hello.sh. RUN chmod +x /hello.sh sets the execution bit on the hello.sh script to make it executable. Listing the image should show that image ID and size have changed.","title":"Build your container again"},{"location":"docs/topics/docker/lesson-02/lesson/#list-available-docker-images_1","text":"Running docker images should produce output similar to: REPOSITORY TAG IMAGE ID CREATED SIZE hello latest 2a56d704ff5b 43 seconds ago 1.22MB <none> <none> f94453b3174d 9 minutes ago 34B busybox latest be5888e67be6 2 weeks ago 1.22MB We should be able to run our container now.","title":"List available docker images"},{"location":"docs/topics/docker/lesson-02/lesson/#run-your-rebuilt-image","text":"docker run --rm hello /hello.sh <%= $env:Username %> Output should be similar to: hello, world <%= $env:Username %>","title":"Run your rebuilt image"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-5-versioning-your-image","text":"Let's roll out a new version of our script","title":"Exercise 5 - Versioning your image"},{"location":"docs/topics/docker/lesson-02/lesson/#modify-your-hello-world-script","text":"echo -e '''#!/bin/sh echo \"hello, world $@\" echo \"I am a new image\" ''' > hello.sh We don't need to update our Dockerfile, as it references the same script. We just have to rebuild the image.","title":"Modify your hello world script"},{"location":"docs/topics/docker/lesson-02/lesson/#build-your-image-again-this-time-with-a-new-tag","text":"docker build -t hello:v2 .","title":"Build your image again, this time with a new tag"},{"location":"docs/topics/docker/lesson-02/lesson/#run-your-v2-image","text":"docker run --rm hello:v2 /hello.sh <%= $env:Username %> Output should be similar to: hello, world <%= $env:Username %> I'm a new image","title":"Run your v2 image"},{"location":"docs/topics/docker/lesson-02/lesson/#exercise-6-entry-points","text":"We can improve our image by supplying an entrypoint. This will set the default command executed if none is specified when starting the container.","title":"Exercise 6 - Entry points"},{"location":"docs/topics/docker/lesson-02/lesson/#define-an-entrypoint-for-your-image","text":"echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile","title":"Define an entrypoint for your image"},{"location":"docs/topics/docker/lesson-02/lesson/#build-a-new-version-of-your-image","text":"docker build -t hello:v3 . We should now be able to run the new image version without supplying additional arguments","title":"Build a new version of your image"},{"location":"docs/topics/docker/lesson-02/lesson/#run-your-v3-image","text":"docker run --rm hello:v3 Output should be similar to: hello, world I am a new image What happens if you pass an additional argument as in previous examples? These will be passed to the ENTRYPOINT command as arguments.","title":"Run your v3 image"},{"location":"docs/topics/docker/lesson-02/lesson/#run-your-v3-image-with-arguments","text":"docker run --rm hello:v3 testing Read more: workshop/docker.md at master \u00b7 gravitational/workshop","title":"Run your v3 image with arguments"},{"location":"docs/topics/docker/lesson-03/lesson/","text":"Forward \u00b6 Now that you have built docker images using the bare minimum approach, let's go over more advanced build topics. The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Exercise 1 - Build and Runtime Environment variables \u00b6 We can pass build and runtime environment variables to our images/containers. Modify your hello.sh shell script \u00b6 echo -e '''#!/bin/sh echo \"hello, $BUILD1 and $RUN1!\" ''' > hello.sh Dockerfile uses the ENV directive to provide environment variable: Modify your Dockerfile \u00b6 echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ENV BUILD1 Bob ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile Build and run your modified image \u00b6 docker build -t hello:v4 . Test Runtime Env Variables \u00b6 docker run --rm -e RUN1=Alice hello:v4 Output should be similar to: hello, Bob and Alice! Note: variables specified at runtime take precedence over those specified at build time, i.e. By overriding the build time variable during runtime, as with: docker run --rm -e BUILD1=Jon -e RUN1=Alice hello:v4 you'll no doubt verify the behavior: hello, Jon and Alice! Exercise 2 - Build Arguments \u00b6 Sometimes it is helpful to supply arguments during the build process, e.g. when user ID needs to be created inside the container. In a nutshell:: The .env file, is only used during a pre-processing step when working with docker-compose.yml files. Dollar-notation variables like $HI are substituted for values contained in an .env named file in the same directory. ARG is only available during the build of a Docker image (RUN etc), not after the image is created and containers are started from it (ENTRYPOINT, CMD). You can use ARG values to set ENV values to workaround that. ENV values are available to containers and to RUN -style commands during the Docker build starting with the line where they are introduced. Setting an environment variable in an intermediate container using bash (e.g. RUN export VARI=5 && ...) will not persist in the next command without a workaround ... An env_file , is a convenient way to pass many environment variables to a single command in one batch. This should not be confused with a .env file Setting ARG and ENV values leaves traces in the Docker image Don't use them for secrets which are not meant to stick around Default and dynamically-set ARG values can be looked at by other people after the image is built. For example, by using the docker history command We can supply build arguments as flags to the docker build command as we already did with the docker run command, as follows ... Update your Dockerfile to allow Build Arguments \u00b6 echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ARG BUILD1 ENV BUILD1=$BUILD1 ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile Build your new image \u00b6 docker build --build-arg BUILD1=\"Bob\" -t hello:v5 . Run your new image \u00b6 docker run --rm -e RUN1=Alice hello:v5 Output should be similar to: hello, Bob and Alice! Read more: How To Pass Environment Info During Docker Builds - Bits and Pieces Exercise 3 - Build Layers and Caching \u00b6 Packaging can often be slow, and Docker builds are no exception. Downloading and installing system and application packages, compiling C extensions, building assets -- it all adds up. In order to speed up your builds, Docker implements caching , i.e. if your Dockerfile and related files haven't changed, a rebuild can reuse some of the existing layers in your local image cache. But in order to take advantage of this cache, you need to understand how it works. When you build a Dockerfile, Docker will see if it can use the cached results of previous builds - For most commands, if the text of the command hasn't changed, the version from the cache will be used. - For COPY , it also checks that the files you're copying haven't changed. Let's create a new workspace and run through an example. Create a new docker workspace \u00b6 mkdir ~/docker-workspace-caching cd ~/docker-workspace-caching Create your support files \u00b6 echo -e '''flask ''' > requirements.txt echo -e '''from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"Hello World!\" if __name__ == \"__main__\": app.run() ''' > server.py Create your Dockerfile \u00b6 echo -e '''FROM python:3.7-slim-buster COPY . . RUN pip install --quiet -r requirements.txt ENTRYPOINT [\"python\", \"server.py\"] ''' > Dockerfile Build your image \u00b6 Observe the output the first time we run our build command docker build -t caching-example1 . Output should be similar to: Sending build context to Docker daemon 5.12kB Step 1/4 : FROM python:3.7-slim-buster ---> f96c28b7013f Step 2/4 : COPY . . ---> eff791eb839d Step 3/4 : RUN pip install --quiet -r requirements.txt ---> Running in 591f97f47b6e Removing intermediate container 591f97f47b6e ---> 02c7cf5a3d9a Step 4/4 : ENTRYPOINT [\"python\", \"server.py\"] ---> Running in e3cf483c3381 Removing intermediate container e3cf483c3381 ---> 598b0340cc90 Successfully built 598b0340cc90 Successfully tagged example1:latest Build your image a second time \u00b6 Let's try building the docker image again docker build -t caching-example1 . Output should be similar to: Sending build context to Docker daemon 5.12kB Step 1/4 : FROM python:3.7-slim-buster ---> f96c28b7013f Step 2/4 : COPY . . ---> Using cache ---> eff791eb839d Step 3/4 : RUN pip install --quiet -r requirements.txt ---> Using cache ---> 02c7cf5a3d9a Step 4/4 : ENTRYPOINT [\"python\", \"server.py\"] ---> Using cache ---> 598b0340cc90 Successfully built 598b0340cc90 Successfully tagged example1:latest Notice it mentions Using cache That's because docker is utilizing the cache, as nothing in the build has changed. The result is a much faster build, since we don't have to download any packages from the network to get pip install to work. To make sure we always take advantage of this great feature, we must be careful to avoid what's known as Cache Invalidation Cache Invalidation \u00b6 Cache Invalidation means: If the cache can't be used for a particular layer, then ALL subsequent layers won't be loaded from the cache Consider the below illustration: | Old Dockerfile | New Dockerfile | Use Cache? | | |:---------------|:---------------|:----------:|:--------------------------------------------| | A | A | Yes | A == A | | B | B_CHANGED | No | B!=B_CHANGED | | C | C | No | No, previous layer wasn't loaded from cache | Notice that the C layer hasn't changed between new and old Dockerfiles Nonetheless , it still can't be loaded from the cache Why? Because the previous layer ( B_CHANGED ) couldn't be loaded from the cache If any of the files we COPY in change, that invalidates all later layers: we'll need to rerun pip install, for example. But if server.py has changed but requirements.txt hasn't, why should we have to redo the pip install? After all, the pip install only uses requirements.txt . What you want to do therefore is to copy only those files that you actually need in order to run the next step, so as to minimize the opportunity for cache invalidation. Because server.py is only copied in after the pip install , the layer created by pip install can still be loaded from the cache so long as requirements.txt hasn't changed. Cache Invalidation - Summary \u00b6 If you want fast builds by reusing your previously cached builds, you'll need to write your Dockerfile appropriately: Only copy in the files you need for the next step, to minimize cache invalidation in the build process. Make sure not to invalidate the cache accidentally by having a command early in the Dockerfile that always changes. Examples of such commands are: a LABEL that contains the build timestamp an ENV directive that changes based on ARG Read More: Production-ready Docker packaging Stuck on Docker packaging? Get up to speed quickly Faster or slower: the basics of Docker build caching","title":"lesson-03"},{"location":"docs/topics/docker/lesson-03/lesson/#forward","text":"Now that you have built docker images using the bare minimum approach, let's go over more advanced build topics.","title":"Forward"},{"location":"docs/topics/docker/lesson-03/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/docker/lesson-03/lesson/#exercise-1-build-and-runtime-environment-variables","text":"We can pass build and runtime environment variables to our images/containers.","title":"Exercise 1 - Build and Runtime Environment variables"},{"location":"docs/topics/docker/lesson-03/lesson/#modify-your-hellosh-shell-script","text":"echo -e '''#!/bin/sh echo \"hello, $BUILD1 and $RUN1!\" ''' > hello.sh Dockerfile uses the ENV directive to provide environment variable:","title":"Modify your hello.sh shell script"},{"location":"docs/topics/docker/lesson-03/lesson/#modify-your-dockerfile","text":"echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ENV BUILD1 Bob ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile","title":"Modify your Dockerfile"},{"location":"docs/topics/docker/lesson-03/lesson/#build-and-run-your-modified-image","text":"docker build -t hello:v4 .","title":"Build and run your modified image"},{"location":"docs/topics/docker/lesson-03/lesson/#test-runtime-env-variables","text":"docker run --rm -e RUN1=Alice hello:v4 Output should be similar to: hello, Bob and Alice! Note: variables specified at runtime take precedence over those specified at build time, i.e. By overriding the build time variable during runtime, as with: docker run --rm -e BUILD1=Jon -e RUN1=Alice hello:v4 you'll no doubt verify the behavior: hello, Jon and Alice!","title":"Test Runtime Env Variables"},{"location":"docs/topics/docker/lesson-03/lesson/#exercise-2-build-arguments","text":"Sometimes it is helpful to supply arguments during the build process, e.g. when user ID needs to be created inside the container. In a nutshell:: The .env file, is only used during a pre-processing step when working with docker-compose.yml files. Dollar-notation variables like $HI are substituted for values contained in an .env named file in the same directory. ARG is only available during the build of a Docker image (RUN etc), not after the image is created and containers are started from it (ENTRYPOINT, CMD). You can use ARG values to set ENV values to workaround that. ENV values are available to containers and to RUN -style commands during the Docker build starting with the line where they are introduced. Setting an environment variable in an intermediate container using bash (e.g. RUN export VARI=5 && ...) will not persist in the next command without a workaround ... An env_file , is a convenient way to pass many environment variables to a single command in one batch. This should not be confused with a .env file Setting ARG and ENV values leaves traces in the Docker image Don't use them for secrets which are not meant to stick around Default and dynamically-set ARG values can be looked at by other people after the image is built. For example, by using the docker history command We can supply build arguments as flags to the docker build command as we already did with the docker run command, as follows ...","title":"Exercise 2 - Build Arguments"},{"location":"docs/topics/docker/lesson-03/lesson/#update-your-dockerfile-to-allow-build-arguments","text":"echo -e '''FROM busybox ADD hello.sh /hello.sh RUN chmod +x /hello.sh ARG BUILD1 ENV BUILD1=$BUILD1 ENTRYPOINT [\"/hello.sh\"] ''' > Dockerfile","title":"Update your Dockerfile to allow Build Arguments"},{"location":"docs/topics/docker/lesson-03/lesson/#build-your-new-image","text":"docker build --build-arg BUILD1=\"Bob\" -t hello:v5 .","title":"Build your new image"},{"location":"docs/topics/docker/lesson-03/lesson/#run-your-new-image","text":"docker run --rm -e RUN1=Alice hello:v5 Output should be similar to: hello, Bob and Alice! Read more: How To Pass Environment Info During Docker Builds - Bits and Pieces","title":"Run your new image"},{"location":"docs/topics/docker/lesson-03/lesson/#exercise-3-build-layers-and-caching","text":"Packaging can often be slow, and Docker builds are no exception. Downloading and installing system and application packages, compiling C extensions, building assets -- it all adds up. In order to speed up your builds, Docker implements caching , i.e. if your Dockerfile and related files haven't changed, a rebuild can reuse some of the existing layers in your local image cache. But in order to take advantage of this cache, you need to understand how it works. When you build a Dockerfile, Docker will see if it can use the cached results of previous builds - For most commands, if the text of the command hasn't changed, the version from the cache will be used. - For COPY , it also checks that the files you're copying haven't changed. Let's create a new workspace and run through an example.","title":"Exercise 3 - Build Layers and Caching"},{"location":"docs/topics/docker/lesson-03/lesson/#create-a-new-docker-workspace","text":"mkdir ~/docker-workspace-caching cd ~/docker-workspace-caching","title":"Create a new docker workspace"},{"location":"docs/topics/docker/lesson-03/lesson/#create-your-support-files","text":"echo -e '''flask ''' > requirements.txt echo -e '''from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"Hello World!\" if __name__ == \"__main__\": app.run() ''' > server.py","title":"Create your support files"},{"location":"docs/topics/docker/lesson-03/lesson/#create-your-dockerfile","text":"echo -e '''FROM python:3.7-slim-buster COPY . . RUN pip install --quiet -r requirements.txt ENTRYPOINT [\"python\", \"server.py\"] ''' > Dockerfile","title":"Create your Dockerfile"},{"location":"docs/topics/docker/lesson-03/lesson/#build-your-image","text":"Observe the output the first time we run our build command docker build -t caching-example1 . Output should be similar to: Sending build context to Docker daemon 5.12kB Step 1/4 : FROM python:3.7-slim-buster ---> f96c28b7013f Step 2/4 : COPY . . ---> eff791eb839d Step 3/4 : RUN pip install --quiet -r requirements.txt ---> Running in 591f97f47b6e Removing intermediate container 591f97f47b6e ---> 02c7cf5a3d9a Step 4/4 : ENTRYPOINT [\"python\", \"server.py\"] ---> Running in e3cf483c3381 Removing intermediate container e3cf483c3381 ---> 598b0340cc90 Successfully built 598b0340cc90 Successfully tagged example1:latest","title":"Build your image"},{"location":"docs/topics/docker/lesson-03/lesson/#build-your-image-a-second-time","text":"Let's try building the docker image again docker build -t caching-example1 . Output should be similar to: Sending build context to Docker daemon 5.12kB Step 1/4 : FROM python:3.7-slim-buster ---> f96c28b7013f Step 2/4 : COPY . . ---> Using cache ---> eff791eb839d Step 3/4 : RUN pip install --quiet -r requirements.txt ---> Using cache ---> 02c7cf5a3d9a Step 4/4 : ENTRYPOINT [\"python\", \"server.py\"] ---> Using cache ---> 598b0340cc90 Successfully built 598b0340cc90 Successfully tagged example1:latest Notice it mentions Using cache That's because docker is utilizing the cache, as nothing in the build has changed. The result is a much faster build, since we don't have to download any packages from the network to get pip install to work. To make sure we always take advantage of this great feature, we must be careful to avoid what's known as Cache Invalidation","title":"Build your image a second time"},{"location":"docs/topics/docker/lesson-03/lesson/#cache-invalidation","text":"Cache Invalidation means: If the cache can't be used for a particular layer, then ALL subsequent layers won't be loaded from the cache Consider the below illustration: | Old Dockerfile | New Dockerfile | Use Cache? | | |:---------------|:---------------|:----------:|:--------------------------------------------| | A | A | Yes | A == A | | B | B_CHANGED | No | B!=B_CHANGED | | C | C | No | No, previous layer wasn't loaded from cache | Notice that the C layer hasn't changed between new and old Dockerfiles Nonetheless , it still can't be loaded from the cache Why? Because the previous layer ( B_CHANGED ) couldn't be loaded from the cache If any of the files we COPY in change, that invalidates all later layers: we'll need to rerun pip install, for example. But if server.py has changed but requirements.txt hasn't, why should we have to redo the pip install? After all, the pip install only uses requirements.txt . What you want to do therefore is to copy only those files that you actually need in order to run the next step, so as to minimize the opportunity for cache invalidation. Because server.py is only copied in after the pip install , the layer created by pip install can still be loaded from the cache so long as requirements.txt hasn't changed.","title":"Cache Invalidation"},{"location":"docs/topics/docker/lesson-03/lesson/#cache-invalidation-summary","text":"If you want fast builds by reusing your previously cached builds, you'll need to write your Dockerfile appropriately: Only copy in the files you need for the next step, to minimize cache invalidation in the build process. Make sure not to invalidate the cache accidentally by having a command early in the Dockerfile that always changes. Examples of such commands are: a LABEL that contains the build timestamp an ENV directive that changes based on ARG Read More: Production-ready Docker packaging Stuck on Docker packaging? Get up to speed quickly Faster or slower: the basics of Docker build caching","title":"Cache Invalidation - Summary"},{"location":"docs/topics/docker/lesson-04/lesson/","text":"Forward \u00b6 The basic idea behind containers is a set of Linux resources that run isolated from the rest of the host OS. In essence, a container is a combination of a few technologies including namespaces, cgroups, and capabilities. You'll dig into the internals of these in the next set of exercises. But first, some concepts ... The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Exercise 1 - Container foundations - Namespaces \u00b6 Purpose of each namespace: Wrap a particular global system resource in an abstraction Make it appear to the processes within the namespace that they have their own isolated instance of the global resource There are six types of namespaces in Linux: Pid : Isolates process identifiers. For example, two processes in different namespaces can have the same PID. User : Isolates user ids and group ids. Two users in two different user namespaces can have the same user ids. Allows mapping an unprivileged user id outside of the namespace to be root inside of the namespace. Net : This namespace provides network isolation. Processes running in a separate net namespace don't see the network interfaces of other namespaces. Mnt : The Mnt namespace creates a scoped view of a filesystem using VFS It allows containers to have their own mount points without polluting the global namespace. It also provides a way to hide the global mount points from other containers. Uts : This allows a container to have its own hostname for the processes running in the container. Ipc : Gives containers their own inter-process communication namespace. Read more: Container Creation Using Namespaces and Bash Namespaces in operation, part 1: namespaces overview Use the VFS storage driver | Docker Documentation Exercise 2 - Container foundations - Control groups \u00b6 Control Groups (also called cgroups) are part of a kernel feature that limits, accounts for, and isolates resources usage (CPU, memory, disk I/O, network, etc.) This feature is particularly useful to predict and plan for enough resources to accommodate the desired number of containers on your systems. Read more: Container Creation Using Namespaces and Bash - DEV Community Exercise 3 - Container foundations - Capabilities \u00b6 Capabilities provide enhanced permission checks on the running process, and can limit the interface configuration, even for a root user. For example, if CAP_NET_ADMIN is disabled, users inside a container (including root) won't be able to manage network interfaces (add, delete, change), change network routes and so on. Read more: Container Creation Using Namespaces and Bash - DEV Community Creating containers - Part 1","title":"lesson-04"},{"location":"docs/topics/docker/lesson-04/lesson/#forward","text":"The basic idea behind containers is a set of Linux resources that run isolated from the rest of the host OS. In essence, a container is a combination of a few technologies including namespaces, cgroups, and capabilities. You'll dig into the internals of these in the next set of exercises. But first, some concepts ...","title":"Forward"},{"location":"docs/topics/docker/lesson-04/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/docker/lesson-04/lesson/#exercise-1-container-foundations-namespaces","text":"Purpose of each namespace: Wrap a particular global system resource in an abstraction Make it appear to the processes within the namespace that they have their own isolated instance of the global resource There are six types of namespaces in Linux: Pid : Isolates process identifiers. For example, two processes in different namespaces can have the same PID. User : Isolates user ids and group ids. Two users in two different user namespaces can have the same user ids. Allows mapping an unprivileged user id outside of the namespace to be root inside of the namespace. Net : This namespace provides network isolation. Processes running in a separate net namespace don't see the network interfaces of other namespaces. Mnt : The Mnt namespace creates a scoped view of a filesystem using VFS It allows containers to have their own mount points without polluting the global namespace. It also provides a way to hide the global mount points from other containers. Uts : This allows a container to have its own hostname for the processes running in the container. Ipc : Gives containers their own inter-process communication namespace. Read more: Container Creation Using Namespaces and Bash Namespaces in operation, part 1: namespaces overview Use the VFS storage driver | Docker Documentation","title":"Exercise 1 - Container foundations - Namespaces"},{"location":"docs/topics/docker/lesson-04/lesson/#exercise-2-container-foundations-control-groups","text":"Control Groups (also called cgroups) are part of a kernel feature that limits, accounts for, and isolates resources usage (CPU, memory, disk I/O, network, etc.) This feature is particularly useful to predict and plan for enough resources to accommodate the desired number of containers on your systems. Read more: Container Creation Using Namespaces and Bash - DEV Community","title":"Exercise 2 - Container foundations - Control groups"},{"location":"docs/topics/docker/lesson-04/lesson/#exercise-3-container-foundations-capabilities","text":"Capabilities provide enhanced permission checks on the running process, and can limit the interface configuration, even for a root user. For example, if CAP_NET_ADMIN is disabled, users inside a container (including root) won't be able to manage network interfaces (add, delete, change), change network routes and so on. Read more: Container Creation Using Namespaces and Bash - DEV Community Creating containers - Part 1","title":"Exercise 3 - Container foundations - Capabilities"},{"location":"docs/topics/go/lesson-01/lesson/","text":"","title":"Lesson"},{"location":"docs/topics/kubernetes/lesson-01/lesson/","text":"Forward \u00b6 Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Kubernetes . The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Lesson 01 \u00b6 This lesson will cover the following exercises: Installing kind Installing helm Initialize a local Kubernetes cluster using kind What is Kubernetes? \u00b6 Kubernetes is a microservices architecture (MSA) platform for automating deployment, operations and scaling of containerized applications. It can run anywhere where Linux runs and supports on-premise, hybrid and public cloud deployments. Exercise 1 - Install prerequisites \u00b6 Install Kind \u00b6 We'll be using kind to initialize our demo cluster. Below are several means of installing the kind binary Install on Linux \u00b6 export KIND_VERSION=v0.14.0 curl -fLo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-linux-amd64\" \\ && chmod +x ./kind \\ && mv ./kind /usr/local/bin/kind Install on OSX (ARM) \u00b6 If you have homebrew installed, you can simply run brew install kind . Otherwise you download via curl : export KIND_VERSION=v0.14.0 curl -fLo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-darwin-arm64\" \\ && chmod +x ./kind \\ && mv ./kind /usr/local/bin/kind Install on Windows \u00b6 On Windows, we can use chocolatey to install: choco install kind Manually download binary \u00b6 You can manually download the kind binary from https://github.com/kubernetes-sigs/kind/releases to get started Install helm \u00b6 Below are several means of installing the helm binary Install on Linux \u00b6 export HELM_VERSION=v3.9.2 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\ && chmod +x get_helm.sh && DESIRED_VERSION=$HELM_VERSION ./get_helm.sh Install on OSX (ARM) \u00b6 If you have homebrew installed, you can simply run brew install helm . Install on Windows \u00b6 On Windows, we can use chocolatey to install: choco install helm Manually download binary \u00b6 You can manually download the kind binary from https://github.com/helm/helm/releases to get started Exercise 2 - Initialize a local Kubernetes cluster \u00b6 For those of you not familiar with the tool, it's used for running local Kubernetes clusters using Docker containers as worker nodes . I've already installed version v0.14.0 of the tool: kind version The syntax for creating a Kubernetes cluster using the kind command is quite simple. We can run kind create cluster --help to get more info, but you can create a cluster by just running kind create cluster . For our demonstration, we'll create a Kubernetes cluster named k8s-demo with some customizations echo -e \"\"\" kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - role: worker \"\"\" | kind create cluster --name k8s-demo --config=- Next, we'll initialize our Kubernetes config and context. You can retrieve your cluster's kubeconfig via the command kind --name k8s-demo get kubeconfig We'll pipe that to our kubeconfig file using the tee command: kind --name k8s-demo get kubeconfig | tee ~/.kube/k8s-demo.yaml We'll then set our KUBECONFIG environmental variable to read any yaml files under our ~/.kube directory: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') then set our active Kubernetes context: kubectl config use-context kind-k8s-demo Verify things are working with: kubectl cluster-info You should now be ready to start playing with Kubernetes.","title":"lesson-01"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#forward","text":"Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Kubernetes .","title":"Forward"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#lesson-01","text":"This lesson will cover the following exercises: Installing kind Installing helm Initialize a local Kubernetes cluster using kind","title":"Lesson 01"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#what-is-kubernetes","text":"Kubernetes is a microservices architecture (MSA) platform for automating deployment, operations and scaling of containerized applications. It can run anywhere where Linux runs and supports on-premise, hybrid and public cloud deployments.","title":"What is Kubernetes?"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#exercise-1-install-prerequisites","text":"","title":"Exercise 1 - Install prerequisites"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-kind","text":"We'll be using kind to initialize our demo cluster. Below are several means of installing the kind binary","title":"Install Kind"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-linux","text":"export KIND_VERSION=v0.14.0 curl -fLo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-linux-amd64\" \\ && chmod +x ./kind \\ && mv ./kind /usr/local/bin/kind","title":"Install on Linux"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-osx-arm","text":"If you have homebrew installed, you can simply run brew install kind . Otherwise you download via curl : export KIND_VERSION=v0.14.0 curl -fLo ./kind \"https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-darwin-arm64\" \\ && chmod +x ./kind \\ && mv ./kind /usr/local/bin/kind","title":"Install on OSX (ARM)"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-windows","text":"On Windows, we can use chocolatey to install: choco install kind","title":"Install on Windows"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#manually-download-binary","text":"You can manually download the kind binary from https://github.com/kubernetes-sigs/kind/releases to get started","title":"Manually download binary"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-helm","text":"Below are several means of installing the helm binary","title":"Install helm"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-linux_1","text":"export HELM_VERSION=v3.9.2 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\ && chmod +x get_helm.sh && DESIRED_VERSION=$HELM_VERSION ./get_helm.sh","title":"Install on Linux"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-osx-arm_1","text":"If you have homebrew installed, you can simply run brew install helm .","title":"Install on OSX (ARM)"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#install-on-windows_1","text":"On Windows, we can use chocolatey to install: choco install helm","title":"Install on Windows"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#manually-download-binary_1","text":"You can manually download the kind binary from https://github.com/helm/helm/releases to get started","title":"Manually download binary"},{"location":"docs/topics/kubernetes/lesson-01/lesson/#exercise-2-initialize-a-local-kubernetes-cluster","text":"For those of you not familiar with the tool, it's used for running local Kubernetes clusters using Docker containers as worker nodes . I've already installed version v0.14.0 of the tool: kind version The syntax for creating a Kubernetes cluster using the kind command is quite simple. We can run kind create cluster --help to get more info, but you can create a cluster by just running kind create cluster . For our demonstration, we'll create a Kubernetes cluster named k8s-demo with some customizations echo -e \"\"\" kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - role: worker \"\"\" | kind create cluster --name k8s-demo --config=- Next, we'll initialize our Kubernetes config and context. You can retrieve your cluster's kubeconfig via the command kind --name k8s-demo get kubeconfig We'll pipe that to our kubeconfig file using the tee command: kind --name k8s-demo get kubeconfig | tee ~/.kube/k8s-demo.yaml We'll then set our KUBECONFIG environmental variable to read any yaml files under our ~/.kube directory: export KUBECONFIG=$(ls ~/.kube/*.yaml | tr '\\n' ':') then set our active Kubernetes context: kubectl config use-context kind-k8s-demo Verify things are working with: kubectl cluster-info You should now be ready to start playing with Kubernetes.","title":"Exercise 2 - Initialize a local Kubernetes cluster"},{"location":"docs/topics/kubernetes/lesson-02/lesson/","text":"Forward \u00b6 Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Kubernetes . The Web Terminal \u00b6 If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here . Lesson 02 \u00b6 This lesson will cover the following exercises: Deploying a hello world service to your Kubernetes cluster, which includes: Deployment Service Ingress","title":"lesson-02"},{"location":"docs/topics/kubernetes/lesson-02/lesson/#forward","text":"Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Kubernetes .","title":"Forward"},{"location":"docs/topics/kubernetes/lesson-02/lesson/#the-web-terminal","text":"If you want to take advantage of the interactive, hands-on nature of these labs, you'll need to either already have a web terminal connection available or fire one up yourself. Instructions for that can be found here .","title":"The Web Terminal"},{"location":"docs/topics/kubernetes/lesson-02/lesson/#lesson-02","text":"This lesson will cover the following exercises: Deploying a hello world service to your Kubernetes cluster, which includes: Deployment Service Ingress","title":"Lesson 02"},{"location":"docs/topics/kubernetes/troubleshooting/docker-volume-mounts-and-autofs/","tags":["kubernetes","docker","microservices","storage"],"text":"Scenario \u00b6 I'm unable to login to my docker host via ssh public key, but my password works, and/or I'm able to login to the system console. Once at the console, I observed an error similar to: Could not chdir to home directory /home/myuser: Too many levels of symbolic links -bash: /home/myuser/.bash_profile: Too many levels of symbolic links Hmm wtf ... Environment details \u00b6 Machine_Type: Virtual OS: Oracle Enterprise Linux 7.x Software: Docker 1.12.6, Kubernetes 1.7.1 Troubleshooting Steps \u00b6 A fellow admin suggested I check for docker mapped volumes that point to /home Here's the command I used to query for that: sudo docker ps --filter volume = /home --format \"Name:\\\\n\\\\t{{.Names}}\\\\nID:\\\\n\\\\t{{.ID}}\\\\nMounts:\\\\n\\\\t{{.Mounts}}\\\\n\" Boom, looks like the kubernetes weaver pod is using that mapping: Name ID Mounts k8s_weave_weave-net-ljzn9_kube-system_740c10c5-d6b8-11e7-838f-005056b5384e_0 dc95801e4442 /opt/kubernetes,/lib/modules,/run/xtables.lo,/var/lib/kubele,/var/lib/weave,/etc,/var/lib/dbus,/var/lib/kubele,/home Ok, so why would a docker volume mapped to /home induce such a problem? Turns out that in some cases, binding autofs-mounted paths to docker containers can cause problems on the docker host. This is due to the way in which kubernetes performs the volume mapping, which utilizes docker volume binds under the hood. And, depending on how you map a volume to a docker container, you might conflict with autofs volume mounting. For insight into a similar issue, see: Issue with AutoFS mounts and docker 1.11.2: https://github.com/moby/moby/issues/24303 According to the issue description above, the problem we're seeing might be fixed by adjusting the bind propagation for the volume mount in question. See: https://docs.docker.com/engine/admin/volumes/bind-mounts/#choosing-the--v-or-mount-flag However, there's no way to control that setting via a kubernetes manifest, not at present at least, since HostPath bind propagation is currently a proposed feature in kubernetes. See: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/propagation.md So the best course of action is to simply change hostPath setting in the weave-kube manifest, e.g. Change: hostPath: path: /home To: hostPath: path: /opt/kubernetes/bind-mounts/weave-kube/home You can then simply redeploy the offending container: kubectl delete daemonset weave-net && kubectl apply -f weave-net.yaml Note: You'll have to perform similar changes to the weave manifest according to whatever other autofs mounts its hostPath(s) might conflict with. Learning Points \u00b6 If you are utilizing autofs on your docker host, ensure you review your autofs settings before deploying your containers.","title":"Docker Volume Mounts And Autofs"},{"location":"docs/topics/kubernetes/troubleshooting/docker-volume-mounts-and-autofs/#scenario","text":"I'm unable to login to my docker host via ssh public key, but my password works, and/or I'm able to login to the system console. Once at the console, I observed an error similar to: Could not chdir to home directory /home/myuser: Too many levels of symbolic links -bash: /home/myuser/.bash_profile: Too many levels of symbolic links Hmm wtf ...","title":"Scenario"},{"location":"docs/topics/kubernetes/troubleshooting/docker-volume-mounts-and-autofs/#environment-details","text":"Machine_Type: Virtual OS: Oracle Enterprise Linux 7.x Software: Docker 1.12.6, Kubernetes 1.7.1","title":"Environment details"},{"location":"docs/topics/kubernetes/troubleshooting/docker-volume-mounts-and-autofs/#troubleshooting-steps","text":"A fellow admin suggested I check for docker mapped volumes that point to /home Here's the command I used to query for that: sudo docker ps --filter volume = /home --format \"Name:\\\\n\\\\t{{.Names}}\\\\nID:\\\\n\\\\t{{.ID}}\\\\nMounts:\\\\n\\\\t{{.Mounts}}\\\\n\" Boom, looks like the kubernetes weaver pod is using that mapping: Name ID Mounts k8s_weave_weave-net-ljzn9_kube-system_740c10c5-d6b8-11e7-838f-005056b5384e_0 dc95801e4442 /opt/kubernetes,/lib/modules,/run/xtables.lo,/var/lib/kubele,/var/lib/weave,/etc,/var/lib/dbus,/var/lib/kubele,/home Ok, so why would a docker volume mapped to /home induce such a problem? Turns out that in some cases, binding autofs-mounted paths to docker containers can cause problems on the docker host. This is due to the way in which kubernetes performs the volume mapping, which utilizes docker volume binds under the hood. And, depending on how you map a volume to a docker container, you might conflict with autofs volume mounting. For insight into a similar issue, see: Issue with AutoFS mounts and docker 1.11.2: https://github.com/moby/moby/issues/24303 According to the issue description above, the problem we're seeing might be fixed by adjusting the bind propagation for the volume mount in question. See: https://docs.docker.com/engine/admin/volumes/bind-mounts/#choosing-the--v-or-mount-flag However, there's no way to control that setting via a kubernetes manifest, not at present at least, since HostPath bind propagation is currently a proposed feature in kubernetes. See: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/propagation.md So the best course of action is to simply change hostPath setting in the weave-kube manifest, e.g. Change: hostPath: path: /home To: hostPath: path: /opt/kubernetes/bind-mounts/weave-kube/home You can then simply redeploy the offending container: kubectl delete daemonset weave-net && kubectl apply -f weave-net.yaml Note: You'll have to perform similar changes to the weave manifest according to whatever other autofs mounts its hostPath(s) might conflict with.","title":"Troubleshooting Steps"},{"location":"docs/topics/kubernetes/troubleshooting/docker-volume-mounts-and-autofs/#learning-points","text":"If you are utilizing autofs on your docker host, ensure you review your autofs settings before deploying your containers.","title":"Learning Points"},{"location":"docs/topics/kubernetes/troubleshooting/pod_tolerates_node_taints/","tags":["kubernetes","microservices"],"text":"Scenario \u00b6 You have a single node (master) kubernetes deployment and you want to schedule standard pods. The master name is your hostname: $(hostname) Upon your attempt at deploying a service, you notice the state of the resulting pod remains in Pending . Further investigation via kubectl describe pod YOUR_POD_NAME reveals an error similar to No nodes are available that match all of the following predicates:: PodToleratesNodeTaints Due Diligence: All kubernetes nodes are in a 'Ready' status: kubectl get nodes All kubernetes nodes have sufficient resources for pod deployment: kubectl describe nodes Your image is available on the docker registry you've specified in your kubernetes manifest (.yaml) Root Cause \u00b6 As per the kubernetes documentation, standard pods are not allowed scheduling on the master node. The mechanism by which this is disallowed is via node taints. You can read more about this here: Taints and Tolerations - Kubernetes Solution \u00b6 You want to be able to schedule a standard pod (i.e. does not belong to the kube-system namespace) on your kubernets master node. As follows: kubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule- You should see a confirmation similar to: node untainted . You should now be able to schedule pods on this node. Notes \u00b6 I came across the github issue description by Googling the following search term: \"No nodes are available that match all of the following predicates\" \"PodToleratesNodeTaints\"","title":"Pod Tolerates Node Taints"},{"location":"docs/topics/kubernetes/troubleshooting/pod_tolerates_node_taints/#scenario","text":"You have a single node (master) kubernetes deployment and you want to schedule standard pods. The master name is your hostname: $(hostname) Upon your attempt at deploying a service, you notice the state of the resulting pod remains in Pending . Further investigation via kubectl describe pod YOUR_POD_NAME reveals an error similar to No nodes are available that match all of the following predicates:: PodToleratesNodeTaints Due Diligence: All kubernetes nodes are in a 'Ready' status: kubectl get nodes All kubernetes nodes have sufficient resources for pod deployment: kubectl describe nodes Your image is available on the docker registry you've specified in your kubernetes manifest (.yaml)","title":"Scenario"},{"location":"docs/topics/kubernetes/troubleshooting/pod_tolerates_node_taints/#root-cause","text":"As per the kubernetes documentation, standard pods are not allowed scheduling on the master node. The mechanism by which this is disallowed is via node taints. You can read more about this here: Taints and Tolerations - Kubernetes","title":"Root Cause"},{"location":"docs/topics/kubernetes/troubleshooting/pod_tolerates_node_taints/#solution","text":"You want to be able to schedule a standard pod (i.e. does not belong to the kube-system namespace) on your kubernets master node. As follows: kubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule- You should see a confirmation similar to: node untainted . You should now be able to schedule pods on this node.","title":"Solution"},{"location":"docs/topics/kubernetes/troubleshooting/pod_tolerates_node_taints/#notes","text":"I came across the github issue description by Googling the following search term: \"No nodes are available that match all of the following predicates\" \"PodToleratesNodeTaints\"","title":"Notes"},{"location":"docs/topics/pfsense/troubleshooting/boot-failure-after-upgrade-to-2-4-0/","tags":["router","firewall","networking","pfSense"],"text":"Scenario \u00b6 You upgraded from pfsense 2.3x to 2.4.0 Upon reboot, I was unable to ssh to the box. Once at the physical console, I noticed pfsense had encountered a panic condition, barking about not being able to mount /dev/ad0s1a Troubleshooting \u00b6 At the prompt, I typed in \"?\" to review the available block devices (disks and the like) I saw in the output the device /dev/ada0s1a , a slightly different device path from what the error message referred to. I then entered in: ufs:/dev/ada0s1a , and boom, pfsense kicked off its regular routines (although it did keep barking about this or that package needing to be cleaned and such). The permanent fix was to correct the mount references in /etc/fstab. I changed any reference to ad0 to ada0 , rebooted, and voila. Next time I upgrade pfsense, I'll read up on any known issues and the like. Hint Hint: 2.4 New Features and Changes: https://doc.pfsense.org/index.php/2.4_New_Features_and_Changes#Known_Issues","title":"Boot failure after upgrading pfsense to 2.4.0"},{"location":"docs/topics/pfsense/troubleshooting/boot-failure-after-upgrade-to-2-4-0/#scenario","text":"You upgraded from pfsense 2.3x to 2.4.0 Upon reboot, I was unable to ssh to the box. Once at the physical console, I noticed pfsense had encountered a panic condition, barking about not being able to mount /dev/ad0s1a","title":"Scenario"},{"location":"docs/topics/pfsense/troubleshooting/boot-failure-after-upgrade-to-2-4-0/#troubleshooting","text":"At the prompt, I typed in \"?\" to review the available block devices (disks and the like) I saw in the output the device /dev/ada0s1a , a slightly different device path from what the error message referred to. I then entered in: ufs:/dev/ada0s1a , and boom, pfsense kicked off its regular routines (although it did keep barking about this or that package needing to be cleaned and such). The permanent fix was to correct the mount references in /etc/fstab. I changed any reference to ad0 to ada0 , rebooted, and voila. Next time I upgrade pfsense, I'll read up on any known issues and the like. Hint Hint: 2.4 New Features and Changes: https://doc.pfsense.org/index.php/2.4_New_Features_and_Changes#Known_Issues","title":"Troubleshooting"},{"location":"docs/topics/terraform/lesson-01/lesson/","text":"Forward \u00b6 Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Terraform . Here's what I will cover for Lab 1 of this Terraform tutorial series: What is Terraform? How to install Terraform on Windows, Linux, and MacOS The basics of Terraform Inputs (variables) and Outputs Let's begin. What is Terraform? \u00b6 Much of the following was taken from the book Terraform In Action from Manning Publications, authored by Scott Winkler. Now, to understand what Terraform is, let's first go over some terminology. Terminology \u00b6 Infrastructure As Code ( IaC ): The process of managing and provisioning infrastructure through machine-readable definition files. We use IaC to automate infrastructure management processes that used to be done manually. Provisioning : The act of deploying infrastructure, as opposed to configuration management (CM), which deals mostly with application delivery and desired state management, particularly on virtual machines (VMs). Infrastructure Provisioning and Configuration Management are inherently different problem domains. Declarative : Say what you want, not how to do it. Example of declarative: Terraform, Ansible Example of imperative: A bash script Even Ansible can be imperative, depending on how you employ it. Mutable Infrastructure : Means you perform software updates on existing servers Immutable Infrastructure : Doesn't care about existing servers -- it treats infrastructure as a disposable commodity. The phrase Pets vs Cattle is often used as an analogy Ok, so what's Terraform? \u00b6 Pardon the repetition, but the concept needs to be hammered in ... Terraform is an infrastructure provisioning tool It is not a Configuration Management (CM) tool Provisioning tools deploy and manage infrastructure, whereas CM tools like Ansible, Puppet, SaltStack, and Chef deploy software and apply configurations onto existing servers. The basic principle of Terraform is that it allows you to write human-readable configuration code to define your IaC. With configuration code, you can deploy repeatable, ephemeral, consistent environments to vendors on the public, private, and hybrid clouds. Some CM tools can also perform a degree of infrastructure provisioning, but not as well as Terraform, because this isn't the task they were originally designed to do. The same can be said the other way around. The difference between CM and provisioning tools is a matter of philosophy CM tools favor mutable infrastructure, whereas Terraform and other provisioning tools favor immutable infrastructure The difference between the two paradigms can be summarized as a reusable versus disposable mentality Terraform IaC is written in HashiCorp Configuration Language (HCL) HCL is fully compatible with JSON. That is, JSON can be used as completely valid input to a system expecting HCL. TL;DR; Terraform is An infrastructure provisioning tool Easy to use Free and open source Declarative Cloud-agnostic Expressive and extensible Now that you have an idea as to how Terraform is used, let's get into its basic concepts and run through some exercises. Concepts & Exercises \u00b6 Before we begin, let's first install Terraform. Installing Terraform \u00b6 Windows \u00b6 It's easiest to install terraform via chocolatey . Once you have chocolatey installed, simply run choco install terraform Linux \u00b6 You can download and extract the terraform binary directly from hashicorp like so: export TERRAFORM_VERSION=1.2.3 curl -kO https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip -d /usr/bin Make sure to adjust the TERRAFORM_VERSION string above to reflect the latest available from the Hashicorp website . Mac OSX \u00b6 For Mac OSX, I recommend you install terraform using homebrew . Simply run brew install terraform . Let's move on to core concepts. Core concepts \u00b6 Order of Operations \u00b6 The major elements of Terraform are: Input variables Resources Data sources (A.K.A read-only resources) Providers Modules Output variables To deploy a Terraform project, you must first organize such elements in configuration code (.tf files) That said, the order of operations for deploying infrastructure via terraform is as follows: Write configuration code (again, .tf files) Initialize workspace with terraform init Invoke terraform plan Apply the configuration with terraform apply ???+ note \":information_source:\" During the plan or apply phase, terraform concatenates all the .tf files in your workspace into a single HCL-formatted file, and automatically determines the correct order in which to derive settings and deploy infrastructure ???+ note \":information_source:\" From the Terraform documentation : The terraform plan command creates an execution plan with a preview of the changes that Terraform will make to your infrastructure. The terraform apply command executes the actions proposed in a Terraform plan. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to approve that plan, before taking the indicated actions. Let's put into practice everything discussed so far with the first piece of our configuration code -- input variables Excercise 1 - Variables \u00b6 As with any modern programming language, variables are basically input values that can change (hence their variable nature), depending on conditions or on information passed in to the program. Like many other approaches to this concept, Terraform employs type constraints , meaning as you declare your input variables, you must also specify their data type. Refer to the Terraform documentation for more information. Let's begin the exercise: Create your workspace: mkdir tf_workspace Change your working directory to your workspace: cd tf_workspace Create the variables.tf file echo -e ''' variable \"my_map\" { type = map(string) default = { \"mykey\" = \"myvar\" } } ''' | tee variables.tf As noted before, this variables file defines our inputs. ???+ note \":information_source:\" You don't have to name the file variables.tf , but it is good practice to do so. Now that we have our variables defined, let's create our outputs file. Exercise 2 - Outputs \u00b6 As with Input variables, Terraform outputs are conceptually the same as with any modern programming language, where for example you may have return values from various functions and output data from the application as a whole. Outputs are meant to be picked up by other Terraform code or even other automation. These outputs are recorded in the Terraform state . Let's begin the exercise: Create the output file echo -e ''' output my_map_dot_notation { value = var.my_map.mykey } output my_map_key_notation { value = var.my_map[\"mykey\"] } ''' | tee outputs.tf Now that your outputs are defined, let's proceed to the last exercise. Exercise 3 - Terraform Plan & Apply \u00b6 As you might have guessed, we're not actually creating any infrastructure in this lesson. We are merely acquainting you with Terraform inputs and outputs. Let's begin the exercise: Initialize the terraform workspace: terraform init During this phase, Terraform will download any externally-sourced modules and initialize any referenced providers. It will also fail early should any problems arise during initialization, e.g. authentication errors, syntax errors, etc. We aren't working with anything mentioned above, so the init will pass with flying colors. Generate the execution plan: terraform plan Because we're only creating outputs, your execution plan should be simple, and similar to: Changes to Outputs: + my_map_dot_notation = \"myvar\" + my_map_key_notation = \"myvar\" Apply the execution plan: terraform apply Answer 'yes' when prompted Again, terraform didn't actually create or destroy anything. Thus, your output should be simple, and similar to: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: my_map_dot_notation = \"myvar\" my_map_key_notation = \"myvar\" Now that you've completed exercise, this concludes the lab.","title":"lesson-01"},{"location":"docs/topics/terraform/lesson-01/lesson/#forward","text":"Thank you for taking the time to start reading this educational material. I hope this hands-on, interactive lesson can reduce the startup cost in your journey to learning Terraform . Here's what I will cover for Lab 1 of this Terraform tutorial series: What is Terraform? How to install Terraform on Windows, Linux, and MacOS The basics of Terraform Inputs (variables) and Outputs Let's begin.","title":"Forward"},{"location":"docs/topics/terraform/lesson-01/lesson/#what-is-terraform","text":"Much of the following was taken from the book Terraform In Action from Manning Publications, authored by Scott Winkler. Now, to understand what Terraform is, let's first go over some terminology.","title":"What is Terraform?"},{"location":"docs/topics/terraform/lesson-01/lesson/#terminology","text":"Infrastructure As Code ( IaC ): The process of managing and provisioning infrastructure through machine-readable definition files. We use IaC to automate infrastructure management processes that used to be done manually. Provisioning : The act of deploying infrastructure, as opposed to configuration management (CM), which deals mostly with application delivery and desired state management, particularly on virtual machines (VMs). Infrastructure Provisioning and Configuration Management are inherently different problem domains. Declarative : Say what you want, not how to do it. Example of declarative: Terraform, Ansible Example of imperative: A bash script Even Ansible can be imperative, depending on how you employ it. Mutable Infrastructure : Means you perform software updates on existing servers Immutable Infrastructure : Doesn't care about existing servers -- it treats infrastructure as a disposable commodity. The phrase Pets vs Cattle is often used as an analogy","title":"Terminology"},{"location":"docs/topics/terraform/lesson-01/lesson/#ok-so-whats-terraform","text":"Pardon the repetition, but the concept needs to be hammered in ... Terraform is an infrastructure provisioning tool It is not a Configuration Management (CM) tool Provisioning tools deploy and manage infrastructure, whereas CM tools like Ansible, Puppet, SaltStack, and Chef deploy software and apply configurations onto existing servers. The basic principle of Terraform is that it allows you to write human-readable configuration code to define your IaC. With configuration code, you can deploy repeatable, ephemeral, consistent environments to vendors on the public, private, and hybrid clouds. Some CM tools can also perform a degree of infrastructure provisioning, but not as well as Terraform, because this isn't the task they were originally designed to do. The same can be said the other way around. The difference between CM and provisioning tools is a matter of philosophy CM tools favor mutable infrastructure, whereas Terraform and other provisioning tools favor immutable infrastructure The difference between the two paradigms can be summarized as a reusable versus disposable mentality Terraform IaC is written in HashiCorp Configuration Language (HCL) HCL is fully compatible with JSON. That is, JSON can be used as completely valid input to a system expecting HCL. TL;DR; Terraform is An infrastructure provisioning tool Easy to use Free and open source Declarative Cloud-agnostic Expressive and extensible Now that you have an idea as to how Terraform is used, let's get into its basic concepts and run through some exercises.","title":"Ok, so what's Terraform?"},{"location":"docs/topics/terraform/lesson-01/lesson/#concepts-exercises","text":"Before we begin, let's first install Terraform.","title":"Concepts &amp; Exercises"},{"location":"docs/topics/terraform/lesson-01/lesson/#installing-terraform","text":"","title":"Installing Terraform"},{"location":"docs/topics/terraform/lesson-01/lesson/#windows","text":"It's easiest to install terraform via chocolatey . Once you have chocolatey installed, simply run choco install terraform","title":"Windows"},{"location":"docs/topics/terraform/lesson-01/lesson/#linux","text":"You can download and extract the terraform binary directly from hashicorp like so: export TERRAFORM_VERSION=1.2.3 curl -kO https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip -d /usr/bin Make sure to adjust the TERRAFORM_VERSION string above to reflect the latest available from the Hashicorp website .","title":"Linux"},{"location":"docs/topics/terraform/lesson-01/lesson/#mac-osx","text":"For Mac OSX, I recommend you install terraform using homebrew . Simply run brew install terraform . Let's move on to core concepts.","title":"Mac OSX"},{"location":"docs/topics/terraform/lesson-01/lesson/#core-concepts","text":"","title":"Core concepts"},{"location":"docs/topics/terraform/lesson-01/lesson/#order-of-operations","text":"The major elements of Terraform are: Input variables Resources Data sources (A.K.A read-only resources) Providers Modules Output variables To deploy a Terraform project, you must first organize such elements in configuration code (.tf files) That said, the order of operations for deploying infrastructure via terraform is as follows: Write configuration code (again, .tf files) Initialize workspace with terraform init Invoke terraform plan Apply the configuration with terraform apply ???+ note \":information_source:\" During the plan or apply phase, terraform concatenates all the .tf files in your workspace into a single HCL-formatted file, and automatically determines the correct order in which to derive settings and deploy infrastructure ???+ note \":information_source:\" From the Terraform documentation : The terraform plan command creates an execution plan with a preview of the changes that Terraform will make to your infrastructure. The terraform apply command executes the actions proposed in a Terraform plan. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to approve that plan, before taking the indicated actions. Let's put into practice everything discussed so far with the first piece of our configuration code -- input variables","title":"Order of Operations"},{"location":"docs/topics/terraform/lesson-01/lesson/#excercise-1-variables","text":"As with any modern programming language, variables are basically input values that can change (hence their variable nature), depending on conditions or on information passed in to the program. Like many other approaches to this concept, Terraform employs type constraints , meaning as you declare your input variables, you must also specify their data type. Refer to the Terraform documentation for more information. Let's begin the exercise: Create your workspace: mkdir tf_workspace Change your working directory to your workspace: cd tf_workspace Create the variables.tf file echo -e ''' variable \"my_map\" { type = map(string) default = { \"mykey\" = \"myvar\" } } ''' | tee variables.tf As noted before, this variables file defines our inputs. ???+ note \":information_source:\" You don't have to name the file variables.tf , but it is good practice to do so. Now that we have our variables defined, let's create our outputs file.","title":"Excercise 1 - Variables"},{"location":"docs/topics/terraform/lesson-01/lesson/#exercise-2-outputs","text":"As with Input variables, Terraform outputs are conceptually the same as with any modern programming language, where for example you may have return values from various functions and output data from the application as a whole. Outputs are meant to be picked up by other Terraform code or even other automation. These outputs are recorded in the Terraform state . Let's begin the exercise: Create the output file echo -e ''' output my_map_dot_notation { value = var.my_map.mykey } output my_map_key_notation { value = var.my_map[\"mykey\"] } ''' | tee outputs.tf Now that your outputs are defined, let's proceed to the last exercise.","title":"Exercise 2 - Outputs"},{"location":"docs/topics/terraform/lesson-01/lesson/#exercise-3-terraform-plan-apply","text":"As you might have guessed, we're not actually creating any infrastructure in this lesson. We are merely acquainting you with Terraform inputs and outputs. Let's begin the exercise: Initialize the terraform workspace: terraform init During this phase, Terraform will download any externally-sourced modules and initialize any referenced providers. It will also fail early should any problems arise during initialization, e.g. authentication errors, syntax errors, etc. We aren't working with anything mentioned above, so the init will pass with flying colors. Generate the execution plan: terraform plan Because we're only creating outputs, your execution plan should be simple, and similar to: Changes to Outputs: + my_map_dot_notation = \"myvar\" + my_map_key_notation = \"myvar\" Apply the execution plan: terraform apply Answer 'yes' when prompted Again, terraform didn't actually create or destroy anything. Thus, your output should be simple, and similar to: Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: my_map_dot_notation = \"myvar\" my_map_key_notation = \"myvar\" Now that you've completed exercise, this concludes the lab.","title":"Exercise 3 - Terraform Plan &amp; Apply"},{"location":"docs/tutorial/advanced/","text":"Advanced \u00b6 Beyond the basic configuration and content Once you've got the Set up Project section, you can customize further using this guide. Or skip this and go to Usage . Navbar nesting \u00b6 You can add an additional level to your navbar like this: nav : - Home : index.md - About : about.md - Foo : - Overview : foo/index.md - Bar : foo/bar.md The value can either be a string (as in the first case) or a map (as in the last case). This seems to be a YAML limitation but see also issue #1139 . Add config options \u00b6 See Configuration page on MkDocs site for options. Separate docs directory approach \u00b6 You can also structure your project to have the set up above nested inside a docs directory. This is useful you have a few other directories and you want to keep the project root clean. docs/ docs/ index.md theme/ main.html nav.html toc.html mkdocs.yml An example of this is the Poetry repo. That project is also how I got into MkDocs in the first place. Embedding \u00b6 To embed a gist, just copy and paste the embed script URL which is provided on a gist. e.g. < script src = \"https://gist.github.com/MichaelCurrin/57caae30bd7b0991098e9804a9494c23.js\" ></ script >","title":"Advanced"},{"location":"docs/tutorial/advanced/#advanced","text":"Beyond the basic configuration and content Once you've got the Set up Project section, you can customize further using this guide. Or skip this and go to Usage .","title":"Advanced"},{"location":"docs/tutorial/advanced/#navbar-nesting","text":"You can add an additional level to your navbar like this: nav : - Home : index.md - About : about.md - Foo : - Overview : foo/index.md - Bar : foo/bar.md The value can either be a string (as in the first case) or a map (as in the last case). This seems to be a YAML limitation but see also issue #1139 .","title":"Navbar nesting"},{"location":"docs/tutorial/advanced/#add-config-options","text":"See Configuration page on MkDocs site for options.","title":"Add config options"},{"location":"docs/tutorial/advanced/#separate-docs-directory-approach","text":"You can also structure your project to have the set up above nested inside a docs directory. This is useful you have a few other directories and you want to keep the project root clean. docs/ docs/ index.md theme/ main.html nav.html toc.html mkdocs.yml An example of this is the Poetry repo. That project is also how I got into MkDocs in the first place.","title":"Separate docs directory approach"},{"location":"docs/tutorial/advanced/#embedding","text":"To embed a gist, just copy and paste the embed script URL which is provided on a gist. e.g. < script src = \"https://gist.github.com/MichaelCurrin/57caae30bd7b0991098e9804a9494c23.js\" ></ script >","title":"Embedding"},{"location":"docs/tutorial/deploy/","text":"Deploy \u00b6 Build and deploy to a remote public site See Deploying Your Docs on the Mkdocs site for more details. GitHub Pages \u00b6 How to deploy your docs site to GitHub Pages. Follow one of the approaches below: Run deploy command - Run a MkDocs CLI command locally to deploy. Deploy with GitHub Actions - Use the project's CI workflow in the cloud to build and deploy to GH Pages on commits pushed to master. Then go to your repo's Settings and Pages then enable GitHub Pages on the gh-pages branch's root. Note this is for a Project Page on a subpath, you will have to make adjustments to the command below if you want an Organization or User Page on the root path. Run deploy command \u00b6 Run a MkDocs CLI command locally to deploy MkDocs needs to know where to publish commits on GitHub - so make sure you are working with a repo that you cloned, or that you initialize the local repo and add a remote repo. Run this command locally: $ make d That will use Makefile to run the following: $ mkdocs gh-deploy --strict --force That will do the following: Clean and build to site directory. Force push to gh-pages branch, overwriting any changes which were pushed from another build. Then go to your repo on GitHub, look at the Environment tab. When it is done building, click View deployment to see your site. e.g. michaelcurrin.github.io/mkdocs-quickstart/ See deploy options in the help: $ mkdocs gh-deploy --help Deploy with GitHub Actions \u00b6 Set up continuous deployment config to enable deploys on a change to files on GitHub When you make changes to your docs config or the docs directory, especially editing on GitHub directly, it's often useful to have the docs site build and deploy automatically in a remote environment. This is provided for free by GitHub. See the docs.yml workflow provided with this project. You don't have to change anything there. The token will be generated for you by GitHub Actions.","title":"Deploy"},{"location":"docs/tutorial/deploy/#deploy","text":"Build and deploy to a remote public site See Deploying Your Docs on the Mkdocs site for more details.","title":"Deploy"},{"location":"docs/tutorial/deploy/#github-pages","text":"How to deploy your docs site to GitHub Pages. Follow one of the approaches below: Run deploy command - Run a MkDocs CLI command locally to deploy. Deploy with GitHub Actions - Use the project's CI workflow in the cloud to build and deploy to GH Pages on commits pushed to master. Then go to your repo's Settings and Pages then enable GitHub Pages on the gh-pages branch's root. Note this is for a Project Page on a subpath, you will have to make adjustments to the command below if you want an Organization or User Page on the root path.","title":"GitHub Pages"},{"location":"docs/tutorial/deploy/#run-deploy-command","text":"Run a MkDocs CLI command locally to deploy MkDocs needs to know where to publish commits on GitHub - so make sure you are working with a repo that you cloned, or that you initialize the local repo and add a remote repo. Run this command locally: $ make d That will use Makefile to run the following: $ mkdocs gh-deploy --strict --force That will do the following: Clean and build to site directory. Force push to gh-pages branch, overwriting any changes which were pushed from another build. Then go to your repo on GitHub, look at the Environment tab. When it is done building, click View deployment to see your site. e.g. michaelcurrin.github.io/mkdocs-quickstart/ See deploy options in the help: $ mkdocs gh-deploy --help","title":"Run deploy command"},{"location":"docs/tutorial/deploy/#deploy-with-github-actions","text":"Set up continuous deployment config to enable deploys on a change to files on GitHub When you make changes to your docs config or the docs directory, especially editing on GitHub directly, it's often useful to have the docs site build and deploy automatically in a remote environment. This is provided for free by GitHub. See the docs.yml workflow provided with this project. You don't have to change anything there. The token will be generated for you by GitHub Actions.","title":"Deploy with GitHub Actions"},{"location":"docs/tutorial/installation/","text":"Installation \u00b6 How to install MkDocs locally. Requirements \u00b6 Python 3 Make - standard on macOS and Linux but can be installed on Windows too. Install system dependencies \u00b6 Set up a new repo \u00b6 Follow the Tutorial page to set up a project from scratch. Or click this create your own copy of the repo. Then clone your repo. e.g. $ git clone git@github.com:MichaelCurrin/mkdocs-quickstart.git $ cd mkdocs-quickstart Install project dependencies \u00b6 Install MkDocs locally For more info, see the Installation page on the MkDocs site. Install in a virtual environment \u00b6 Create a virtual environment at the project root - this is used to isolate project packages from the global packages. $ python3 -m venv venv Activate the environment. $ source venv/bin/activate Install mkdocs - this is covered in the project requirements file. $ cd docs $ make install Note - mkdocs 1.2 causes a break on force pushes, so this is excluded in the requirements. See issue #2447 . Install globally \u00b6 If you prefer to install MkDocs once and reuse it across projects, then you can install it globally instead. MkDocs is available using package managers like apt-get , homebrew and yum . Or you can install like this: $ python3 -m pip install mkdocs If you get prompted for sudo use, then cancel and run again with -U flag for user-level install.","title":"Installation"},{"location":"docs/tutorial/installation/#installation","text":"How to install MkDocs locally.","title":"Installation"},{"location":"docs/tutorial/installation/#requirements","text":"Python 3 Make - standard on macOS and Linux but can be installed on Windows too.","title":"Requirements"},{"location":"docs/tutorial/installation/#install-system-dependencies","text":"","title":"Install system dependencies"},{"location":"docs/tutorial/installation/#set-up-a-new-repo","text":"Follow the Tutorial page to set up a project from scratch. Or click this create your own copy of the repo. Then clone your repo. e.g. $ git clone git@github.com:MichaelCurrin/mkdocs-quickstart.git $ cd mkdocs-quickstart","title":"Set up a new repo"},{"location":"docs/tutorial/installation/#install-project-dependencies","text":"Install MkDocs locally For more info, see the Installation page on the MkDocs site.","title":"Install project dependencies"},{"location":"docs/tutorial/installation/#install-in-a-virtual-environment","text":"Create a virtual environment at the project root - this is used to isolate project packages from the global packages. $ python3 -m venv venv Activate the environment. $ source venv/bin/activate Install mkdocs - this is covered in the project requirements file. $ cd docs $ make install Note - mkdocs 1.2 causes a break on force pushes, so this is excluded in the requirements. See issue #2447 .","title":"Install in a virtual environment"},{"location":"docs/tutorial/installation/#install-globally","text":"If you prefer to install MkDocs once and reuse it across projects, then you can install it globally instead. MkDocs is available using package managers like apt-get , homebrew and yum . Or you can install like this: $ python3 -m pip install mkdocs If you get prompted for sudo use, then cancel and run again with -U flag for user-level install.","title":"Install globally"},{"location":"docs/tutorial/quickstart/","text":"MkDocs Quickstart \u00b6 Started template for a MkDocs docs site on GH Pages - including CI Use the MkDocs ( make docs ) tool to create build a docs site around markdown docs. Follow the tutorial to add an existing project or create a project from scratch. The result will look like this project. Features \u00b6 How to use this project Follow the tutorial instructions Install and configure a new or existing project. Run it locally. Deploy it. Add a copy of this project to your repos View the live demo This site is hosted on GitHub Pages. See if you like it. Other themes are available - see the tutorial. The aim here is not be complete or explain all concepts. It is to provide a reference for common steps and choices needed when setting up a docs site, but still at a beginner-friendly level. This guide is based on the mkdocs.org tutorial.","title":"Quickstart"},{"location":"docs/tutorial/quickstart/#mkdocs-quickstart","text":"Started template for a MkDocs docs site on GH Pages - including CI Use the MkDocs ( make docs ) tool to create build a docs site around markdown docs. Follow the tutorial to add an existing project or create a project from scratch. The result will look like this project.","title":"MkDocs Quickstart"},{"location":"docs/tutorial/quickstart/#features","text":"How to use this project Follow the tutorial instructions Install and configure a new or existing project. Run it locally. Deploy it. Add a copy of this project to your repos View the live demo This site is hosted on GitHub Pages. See if you like it. Other themes are available - see the tutorial. The aim here is not be complete or explain all concepts. It is to provide a reference for common steps and choices needed when setting up a docs site, but still at a beginner-friendly level. This guide is based on the mkdocs.org tutorial.","title":"Features"},{"location":"docs/tutorial/setup-project/","text":"Set up project \u00b6 How to create a MkDocs site from scratch This is a summary of the tutorial on mkdocs.org . How to use this guide \u00b6 Use one of the approaches below: Create a quickstart project with the new command covered in Create a starter site . Follow the extended guide to create a Set up up docs site by hand. Basic structure \u00b6 This is the simplest MkDocs site you can make: docs/ index.md - Homepage in the docs directory (by default). mkdocs.yml Config at the root - control appearance and navigation of your site. See this project's docs/mkdocs.yml file on GitHub. Notes on fields for the config: site_name - title of your site. site_description - used as a description for SEO and you could use it somewhere in your template. site_url - now a required field when running a deploy. Include the subpath on the domain. If your site is not ready to be hosted, set this field to an empty string. repo_url - for Edit on GitHub button. See docs . edit_uri - defaults to edit/master/docs/ , which includes docs directory of markdown files inside your repo root. But, if the site's root is in an outer docs directory, then you need to also add that, so paths do not break. e.g. edit/master/docs/docs . See docs Requirements file \u00b6 A requirements file is optional but it can make it easier to manage dependencies. If you choose not use the file, make sure pip install mkdocs and pip install THEME lines are your instructions. If you want to add, then include requirements.txt at the root. If your project is already a Python project, you might prefer to add mkdocs in requirements-dev.txt or at docs/requirements.txt to keep it isolated.This file should have mkdocs in it and also any themes if needed. Create a starter site \u00b6 Run this command to create a starter site. This make the steps below go quicker. cd my-project mkdocs new PATH The result will be same as the Basic structure defined above and will include minimal text content generated by the MkDocs CLI. This text is defined in the project's new.py module. Set up a docs site \u00b6 Tip: Optionally use the new command covered above to set up the config and index page first and then continue . Create doc pages. Create a docs directory. Create index.md as your homepage. Create other markdown pages (optional). Use placeholder content if you want to move on and then come back to expand them. If you have any existing markdown docs, these will work too. Set up config. Create mkdocs.yml at the project root. Set up a navbar there. Choose a theme. Create a favicon (optional). It will be picked up at this path: docs/img/favicon.ico . Add to your .gitignore . Add build directory. This will prevent it from being versioned on master branch. Add virtual environment, if using one. You project should now look this this: docs/ index.md More pages... mkdocs.yml .gitignore venv requirements.txt - optional Sample content \u00b6 Ignore file \u00b6 .gitignore site/ venv Navbar \u00b6 mkdocs.yml nav : - Home : index.md - About : about.md Themes \u00b6 Builtin \u00b6 Use a builtin theme that comes with MkDocs. The default. theme : mkdocs Using ReadTheDocs theme and alternative config syntax. theme : name : readthedocs Find more supported themes . If it doesn't immediately, you'll have to use pip to install it and add to a requirements.txt file. ReadTheDocs Dropdown theme \u00b6 See below using mkdocs-rtd-dropdown . requirements.txt : mkdocs-rtd-dropdown mkdocs.yml : theme : name : 'rtd-dropdown' Material for MkdDocs theme \u00b6 See the MkDocs for Material homepage. See the Set up page for config options. requirements.txt : mkdocs-material-extensions>=1.0 mkdocs.yml : theme : name : 'material'","title":"Set up project"},{"location":"docs/tutorial/setup-project/#set-up-project","text":"How to create a MkDocs site from scratch This is a summary of the tutorial on mkdocs.org .","title":"Set up project"},{"location":"docs/tutorial/setup-project/#how-to-use-this-guide","text":"Use one of the approaches below: Create a quickstart project with the new command covered in Create a starter site . Follow the extended guide to create a Set up up docs site by hand.","title":"How to use this guide"},{"location":"docs/tutorial/setup-project/#basic-structure","text":"This is the simplest MkDocs site you can make: docs/ index.md - Homepage in the docs directory (by default). mkdocs.yml Config at the root - control appearance and navigation of your site. See this project's docs/mkdocs.yml file on GitHub. Notes on fields for the config: site_name - title of your site. site_description - used as a description for SEO and you could use it somewhere in your template. site_url - now a required field when running a deploy. Include the subpath on the domain. If your site is not ready to be hosted, set this field to an empty string. repo_url - for Edit on GitHub button. See docs . edit_uri - defaults to edit/master/docs/ , which includes docs directory of markdown files inside your repo root. But, if the site's root is in an outer docs directory, then you need to also add that, so paths do not break. e.g. edit/master/docs/docs . See docs","title":"Basic structure"},{"location":"docs/tutorial/setup-project/#requirements-file","text":"A requirements file is optional but it can make it easier to manage dependencies. If you choose not use the file, make sure pip install mkdocs and pip install THEME lines are your instructions. If you want to add, then include requirements.txt at the root. If your project is already a Python project, you might prefer to add mkdocs in requirements-dev.txt or at docs/requirements.txt to keep it isolated.This file should have mkdocs in it and also any themes if needed.","title":"Requirements file"},{"location":"docs/tutorial/setup-project/#create-a-starter-site","text":"Run this command to create a starter site. This make the steps below go quicker. cd my-project mkdocs new PATH The result will be same as the Basic structure defined above and will include minimal text content generated by the MkDocs CLI. This text is defined in the project's new.py module.","title":"Create a starter site"},{"location":"docs/tutorial/setup-project/#set-up-a-docs-site","text":"Tip: Optionally use the new command covered above to set up the config and index page first and then continue . Create doc pages. Create a docs directory. Create index.md as your homepage. Create other markdown pages (optional). Use placeholder content if you want to move on and then come back to expand them. If you have any existing markdown docs, these will work too. Set up config. Create mkdocs.yml at the project root. Set up a navbar there. Choose a theme. Create a favicon (optional). It will be picked up at this path: docs/img/favicon.ico . Add to your .gitignore . Add build directory. This will prevent it from being versioned on master branch. Add virtual environment, if using one. You project should now look this this: docs/ index.md More pages... mkdocs.yml .gitignore venv requirements.txt - optional","title":"Set up a docs site"},{"location":"docs/tutorial/setup-project/#sample-content","text":"","title":"Sample content"},{"location":"docs/tutorial/setup-project/#ignore-file","text":".gitignore site/ venv","title":"Ignore file"},{"location":"docs/tutorial/setup-project/#navbar","text":"mkdocs.yml nav : - Home : index.md - About : about.md","title":"Navbar"},{"location":"docs/tutorial/setup-project/#themes","text":"","title":"Themes"},{"location":"docs/tutorial/setup-project/#builtin","text":"Use a builtin theme that comes with MkDocs. The default. theme : mkdocs Using ReadTheDocs theme and alternative config syntax. theme : name : readthedocs Find more supported themes . If it doesn't immediately, you'll have to use pip to install it and add to a requirements.txt file.","title":"Builtin"},{"location":"docs/tutorial/setup-project/#readthedocs-dropdown-theme","text":"See below using mkdocs-rtd-dropdown . requirements.txt : mkdocs-rtd-dropdown mkdocs.yml : theme : name : 'rtd-dropdown'","title":"ReadTheDocs Dropdown theme"},{"location":"docs/tutorial/setup-project/#material-for-mkddocs-theme","text":"See the MkDocs for Material homepage. See the Set up page for config options. requirements.txt : mkdocs-material-extensions>=1.0 mkdocs.yml : theme : name : 'material'","title":"Material for MkdDocs theme"},{"location":"docs/tutorial/tldr/","text":"TL;DR \u00b6 A simplified version of the tutorial. Local setup \u00b6 Install pip install mkdocs Set up project mkdocs new . or Run mkdocs serve View on localhost:8000 Deploy to remote site \u00b6 Run deploy command locally \u00b6 Deploy to GitHub Pages mkdocs gh-deploy View published site on GitHub Pages at https://USERNAME.github.io/REPO-NAME/ Run continuous integration \u00b6 For CI/CD deploy, use GitHub Actions with an action such as Deploy MkDocs . Or use Netlify. This is not covered in this tutorial.","title":"TL;DR"},{"location":"docs/tutorial/tldr/#tldr","text":"A simplified version of the tutorial.","title":"TL;DR"},{"location":"docs/tutorial/tldr/#local-setup","text":"Install pip install mkdocs Set up project mkdocs new . or Run mkdocs serve View on localhost:8000","title":"Local setup"},{"location":"docs/tutorial/tldr/#deploy-to-remote-site","text":"","title":"Deploy to remote site"},{"location":"docs/tutorial/tldr/#run-deploy-command-locally","text":"Deploy to GitHub Pages mkdocs gh-deploy View published site on GitHub Pages at https://USERNAME.github.io/REPO-NAME/","title":"Run deploy command locally"},{"location":"docs/tutorial/tldr/#run-continuous-integration","text":"For CI/CD deploy, use GitHub Actions with an action such as Deploy MkDocs . Or use Netlify. This is not covered in this tutorial.","title":"Run continuous integration"},{"location":"docs/tutorial/usage/","text":"Usage \u00b6 Build and preview a site locally Make sure to run all commands from the docs directory, as that is where Makefile is. $ cd docs CLI help \u00b6 $ make help default: install all: install build h help: install: upgrade: s serve: b build: d deploy: Serve docs \u00b6 This will build the docs in memory (not to disk) and serve an auto-reloading server. $ make serve Then open in your browser: localhost:8000 Build docs \u00b6 Build docs site to site directory. This is useful for a CI flow. $ make build","title":"Usage"},{"location":"docs/tutorial/usage/#usage","text":"Build and preview a site locally Make sure to run all commands from the docs directory, as that is where Makefile is. $ cd docs","title":"Usage"},{"location":"docs/tutorial/usage/#cli-help","text":"$ make help default: install all: install build h help: install: upgrade: s serve: b build: d deploy:","title":"CLI help"},{"location":"docs/tutorial/usage/#serve-docs","text":"This will build the docs in memory (not to disk) and serve an auto-reloading server. $ make serve Then open in your browser: localhost:8000","title":"Serve docs"},{"location":"docs/tutorial/usage/#build-docs","text":"Build docs site to site directory. This is useful for a CI flow. $ make build","title":"Build docs"}]}